{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb9cfa4",
   "metadata": {},
   "source": [
    "# Task 4.G - Training Model with Ditto (FAIR-DA4ER)\n",
    "\n",
    "This notebook trains a deep learning Entity Resolution model using the Ditto framework from the FAIR-DA4ER repository.\n",
    "\n",
    "**Ditto** uses pre-trained language models (like DistilBERT, RoBERTa) for entity matching. It treats the problem as a sequence pair classification task.\n",
    "\n",
    "## References:\n",
    "- Li et al., \"Deep Entity Matching with Pre-Trained Language Models\"\n",
    "- GitHub: https://github.com/MarcoNapoleone/FAIR-DA4ER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c493632",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db7d729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.10.0+cu128\n",
      "Uninstalling torch-2.10.0+cu128:\n",
      "  Successfully uninstalled torch-2.10.0+cu128\n",
      "Found existing installation: torchvision 0.25.0+cu128\n",
      "Uninstalling torchvision-0.25.0+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0+cu128\n",
      "Found existing installation: torchaudio 2.10.0+cu128\n",
      "Uninstalling torchaudio-2.10.0+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\migli\\HW6ID\\.venv\\Lib\\site-packages\\~-rch'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\migli\\HW6ID\\.venv\\Lib\\site-packages\\~orchvision'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torch-2.10.0%2Bcu128-cp311-cp311-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchvision-0.25.0%2Bcu128-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.10.0%2Bcu128-cp311-cp311-win_amd64.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: filelock in .\\.venv\\Lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in .\\.venv\\Lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in .\\.venv\\Lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in .\\.venv\\Lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\Lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in .\\.venv\\Lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from torchvision) (2.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in .\\.venv\\Lib\\site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\.venv\\Lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\Lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torch-2.10.0%2Bcu128-cp311-cp311-win_amd64.whl (2867.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torchvision-0.25.0%2Bcu128-cp311-cp311-win_amd64.whl (9.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu128/torchaudio-2.10.0%2Bcu128-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   ---------------------------------------- 3/3 [torchaudio]\n",
      "\n",
      "Successfully installed torch-2.10.0+cu128 torchaudio-2.10.0+cu128 torchvision-0.25.0+cu128\n",
      "Requirement already satisfied: transformers in .\\.venv\\Lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: tqdm in .\\.venv\\Lib\\site-packages (4.67.2)\n",
      "Requirement already satisfied: jsonlines in .\\.venv\\Lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: nltk in .\\.venv\\Lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tensorboardX in .\\.venv\\Lib\\site-packages (2.6.4)\n",
      "Requirement already satisfied: scikit-learn in .\\.venv\\Lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: pandas in .\\.venv\\Lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: scipy in .\\.venv\\Lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: filelock in .\\.venv\\Lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in .\\.venv\\Lib\\site-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\Lib\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\Lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\Lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in .\\.venv\\Lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in .\\.venv\\Lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in .\\.venv\\Lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in .\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\Lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in .\\.venv\\Lib\\site-packages (from jsonlines) (25.4.0)\n",
      "Requirement already satisfied: click in .\\.venv\\Lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in .\\.venv\\Lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in .\\.venv\\Lib\\site-packages (from tensorboardX) (6.33.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in .\\.venv\\Lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\Lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch with CUDA 12.8 support (for GPU training)\n",
    "# First uninstall CPU version, then install CUDA version\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n",
    "# Install other Ditto requirements\n",
    "!pip install transformers tqdm jsonlines nltk tensorboardX scikit-learn pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36fbae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DITTO_DIR: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\n",
      "DITTO_DATA_DIR: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\n",
      "DITTO_RESULTS_DIR: c:\\Users\\migli\\HW6ID\\ditto_results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Paths\n",
    "WORKSPACE = r'c:\\Users\\migli\\HW6ID'\n",
    "GROUND_TRUTH_DIR = os.path.join(WORKSPACE, 'ground_truth')\n",
    "DITTO_DIR = os.path.join(WORKSPACE, 'FAIR-DA4ER', 'ditto')\n",
    "BLOCKING_RESULTS_DIR = os.path.join(WORKSPACE, 'blocking_results')\n",
    "\n",
    "# Create output directory for our dataset\n",
    "DITTO_DATA_DIR = os.path.join(DITTO_DIR, 'data', 'cars')\n",
    "os.makedirs(DITTO_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Create results directory\n",
    "DITTO_RESULTS_DIR = os.path.join(WORKSPACE, 'ditto_results')\n",
    "os.makedirs(DITTO_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"DITTO_DIR: {DITTO_DIR}\")\n",
    "print(f\"DITTO_DATA_DIR: {DITTO_DATA_DIR}\")\n",
    "print(f\"DITTO_RESULTS_DIR: {DITTO_RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9eb01",
   "metadata": {},
   "source": [
    "## 2. Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96deba95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A: 14492 records\n",
      "Dataset B: 15375 records\n",
      "\n",
      "Dataset A columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location', 'longitude', 'manufacturer', 'mileage', 'model', 'price', 'source_dataset', 'transmission', 'vehicle_id', 'year']\n",
      "Dataset B columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location', 'longitude', 'manufacturer', 'mileage', 'model', 'price', 'source_dataset', 'transmission', 'vehicle_id', 'year']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets A and B\n",
    "dataset_A = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_A_no_vin.csv'))\n",
    "dataset_B = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_B_no_vin.csv'))\n",
    "\n",
    "print(f\"Dataset A: {len(dataset_A)} records\")\n",
    "print(f\"Dataset B: {len(dataset_B)} records\")\n",
    "print(f\"\\nDataset A columns: {list(dataset_A.columns)}\")\n",
    "print(f\"Dataset B columns: {list(dataset_B.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ace2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10788 pairs, Positives: 2724\n",
      "Validation: 2311 pairs, Positives: 548\n",
      "Test: 2313 pairs, Positives: 581\n"
     ]
    }
   ],
   "source": [
    "# Load train/validation/test sets\n",
    "train_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'validation.csv'))\n",
    "test_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'test.csv'))\n",
    "\n",
    "print(f\"Train: {len(train_df)} pairs, Positives: {train_df['label'].sum()}\")\n",
    "print(f\"Validation: {len(valid_df)} pairs, Positives: {valid_df['label'].sum()}\")\n",
    "print(f\"Test: {len(test_df)} pairs, Positives: {test_df['label'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead8842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns:\n",
      "['vin', 'craigslist_id', 'used_cars_id', 'year', 'manufacturer_cr', 'manufacturer_uc', 'model_cr', 'model_uc', 'price_cr', 'price_uc', 'mileage_cr', 'mileage_uc', 'warnings', 'warning_details', 'label']\n",
      "\n",
      "Sample row:\n",
      "   vin  craigslist_id  used_cars_id  year manufacturer_cr manufacturer_uc  \\\n",
      "0  NaN     7316716231     267497466   NaN       Chevrolet       Chevrolet   \n",
      "1  NaN     7307279962     272550118   NaN       Chevrolet           Mazda   \n",
      "\n",
      "               model_cr model_uc  price_cr  price_uc  mileage_cr  mileage_uc  \\\n",
      "0  Colorado Z71 4X4 Gas    Cruze     39999   19990.0     13303.0         4.0   \n",
      "1              Suburban   Mazda6     23999   33590.0    200754.0         3.0   \n",
      "\n",
      "   warnings warning_details  label  \n",
      "0       NaN             NaN      0  \n",
      "1       NaN             NaN      0  \n"
     ]
    }
   ],
   "source": [
    "# Check train column structure\n",
    "print(\"Train columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nSample row:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c072704",
   "metadata": {},
   "source": [
    "## 3. Convert to Ditto Serialization Format\n",
    "\n",
    "Ditto expects data in the format:\n",
    "```\n",
    "COL attr1 VAL value1 COL attr2 VAL value2 ...\\tCOL attr1 VAL value1 COL attr2 VAL value2 ...\\tlabel\n",
    "```\n",
    "\n",
    "Where each line contains:\n",
    "- Record 1 serialized with COL/VAL markers\n",
    "- TAB separator\n",
    "- Record 2 serialized with COL/VAL markers\n",
    "- TAB separator\n",
    "- Label (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c981b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample row (label=0):\n",
      "COL manufacturer VAL Toyota COL model VAL Avalon Limited Sedan 4D COL price VAL 37590 COL mileage VAL 3939.0\n",
      "---\n",
      "COL manufacturer VAL Toyota COL model VAL Corolla Hybrid COL price VAL 25183.0 COL mileage VAL 3.0\n"
     ]
    }
   ],
   "source": [
    "def serialize_record(row, prefix_cr=True):\n",
    "    \"\"\"\n",
    "    Serialize a record to Ditto format.\n",
    "    Uses the columns from train.csv which have _cr (Craigslist) and _uc (Used Cars) suffixes.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    # Common attributes to serialize for matching\n",
    "    if prefix_cr:\n",
    "        # Craigslist record\n",
    "        attrs = [\n",
    "            ('year', 'year'),  # Use common year column\n",
    "            ('manufacturer', 'manufacturer_cr'),\n",
    "            ('model', 'model_cr'),\n",
    "            ('price', 'price_cr'),\n",
    "            ('mileage', 'mileage_cr'),\n",
    "        ]\n",
    "    else:\n",
    "        # Used Cars record\n",
    "        attrs = [\n",
    "            ('year', 'year'),  # Use common year column\n",
    "            ('manufacturer', 'manufacturer_uc'),\n",
    "            ('model', 'model_uc'),\n",
    "            ('price', 'price_uc'),\n",
    "            ('mileage', 'mileage_uc'),\n",
    "        ]\n",
    "    \n",
    "    for attr_name, col_name in attrs:\n",
    "        if col_name in row.index:\n",
    "            val = row[col_name]\n",
    "            if pd.notna(val):\n",
    "                val_str = str(val).strip()\n",
    "                if val_str:\n",
    "                    tokens.append(f\"COL {attr_name} VAL {val_str}\")\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def convert_to_ditto_format(df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame with paired records to Ditto format.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Converting to Ditto format\"):\n",
    "        rec1 = serialize_record(row, prefix_cr=True)\n",
    "        rec2 = serialize_record(row, prefix_cr=False)\n",
    "        label = int(row['label'])\n",
    "        \n",
    "        line = f\"{rec1}\\t{rec2}\\t{label}\"\n",
    "        lines.append(line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "# Test on a sample\n",
    "sample_row = train_df.iloc[5]  # A positive match\n",
    "print(\"Sample row (label={}):\".format(sample_row['label']))\n",
    "print(serialize_record(sample_row, prefix_cr=True))\n",
    "print(\"---\")\n",
    "print(serialize_record(sample_row, prefix_cr=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea901f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to Ditto format: 100%|██████████| 10788/10788 [00:00<00:00, 44559.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to Ditto format: 100%|██████████| 2311/2311 [00:00<00:00, 46220.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to Ditto format: 100%|██████████| 2313/2313 [00:00<00:00, 46260.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted: train=10788, valid=2311, test=2313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all datasets to Ditto format\n",
    "print(\"Converting train set...\")\n",
    "train_lines = convert_to_ditto_format(train_df)\n",
    "\n",
    "print(\"\\nConverting validation set...\")\n",
    "valid_lines = convert_to_ditto_format(valid_df)\n",
    "\n",
    "print(\"\\nConverting test set...\")\n",
    "test_lines = convert_to_ditto_format(test_df)\n",
    "\n",
    "print(f\"\\nConverted: train={len(train_lines)}, valid={len(valid_lines)}, test={len(test_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051fe532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train.txt: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\train.txt\n",
      "Saved valid.txt: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\valid.txt\n",
      "Saved test.txt: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\test.txt\n",
      "\n",
      "Sample lines from train.txt:\n",
      "COL manufacturer VAL Chevrolet COL model VAL Colorado Z71 4X4 Gas COL price VAL 39999 COL mileage VAL 13303.0\tCOL manufacturer VAL Chevrolet COL model VAL Cruze COL price VAL 19990.0 COL mileage VAL 4.0\t0\n",
      "---\n",
      "COL manufacturer VAL Toyota COL model VAL Avalon Limited Sedan 4D COL price VAL 37590 COL mileage VAL 3939.0\tCOL manufacturer VAL Toyota COL model VAL Corolla Hybrid COL price VAL 25183.0 COL mileage VAL 3.0\t0\n"
     ]
    }
   ],
   "source": [
    "# Save to files\n",
    "train_path = os.path.join(DITTO_DATA_DIR, 'train.txt')\n",
    "valid_path = os.path.join(DITTO_DATA_DIR, 'valid.txt')\n",
    "test_path = os.path.join(DITTO_DATA_DIR, 'test.txt')\n",
    "\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_lines))\n",
    "\n",
    "with open(valid_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(valid_lines))\n",
    "\n",
    "with open(test_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test_lines))\n",
    "\n",
    "print(f\"Saved train.txt: {train_path}\")\n",
    "print(f\"Saved valid.txt: {valid_path}\")\n",
    "print(f\"Saved test.txt: {test_path}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample lines from train.txt:\")\n",
    "print(train_lines[0])\n",
    "print(\"---\")\n",
    "print(train_lines[5])  # A positive example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f41696",
   "metadata": {},
   "source": [
    "## 4. Add Configuration to configs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11550d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars_ER configuration already exists\n",
      "\n",
      "Our config: {'name': 'Cars_ER', 'task_type': 'classification', 'vocab': ['0', '1'], 'trainset': 'data/cars/train.txt', 'validset': 'data/cars/valid.txt', 'testset': 'data/cars/test.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Load existing configs and add our task\n",
    "configs_path = os.path.join(DITTO_DIR, 'configs.json')\n",
    "\n",
    "with open(configs_path, 'r') as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "# Check if our config already exists\n",
    "our_config = {\n",
    "    \"name\": \"Cars_ER\",\n",
    "    \"task_type\": \"classification\",\n",
    "    \"vocab\": [\"0\", \"1\"],\n",
    "    \"trainset\": \"data/cars/train.txt\",\n",
    "    \"validset\": \"data/cars/valid.txt\",\n",
    "    \"testset\": \"data/cars/test.txt\"\n",
    "}\n",
    "\n",
    "# Add if not exists\n",
    "config_names = [c['name'] for c in configs]\n",
    "if 'Cars_ER' not in config_names:\n",
    "    configs.append(our_config)\n",
    "    with open(configs_path, 'w') as f:\n",
    "        json.dump(configs, f, indent=2)\n",
    "    print(\"Added Cars_ER configuration to configs.json\")\n",
    "else:\n",
    "    print(\"Cars_ER configuration already exists\")\n",
    "\n",
    "print(f\"\\nOur config: {our_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a66238",
   "metadata": {},
   "source": [
    "## 5. Train Ditto Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abf13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\n"
     ]
    }
   ],
   "source": [
    "# Change to ditto directory and add to path\n",
    "os.chdir(DITTO_DIR)\n",
    "sys.path.insert(0, DITTO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e29cefc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\migli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\migli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\migli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca1be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "# Import Ditto components\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed43336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migli\\HW6ID\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditto modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Ditto modules\n",
    "from ditto_light.dataset import DittoDataset\n",
    "from ditto_light.ditto import train as ditto_train, evaluate, DittoModel\n",
    "\n",
    "print(\"Ditto modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffac1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'name': 'Cars_ER', 'task_type': 'classification', 'vocab': ['0', '1'], 'trainset': 'data/cars/train.txt', 'validset': 'data/cars/valid.txt', 'testset': 'data/cars/test.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Load configs\n",
    "os.chdir(DITTO_DIR)\n",
    "configs_path = os.path.join(DITTO_DIR, 'configs.json')\n",
    "with open(configs_path, 'r') as f:\n",
    "    configs = json.load(f)\n",
    "configs_dict = {c['name']: c for c in configs}\n",
    "config = configs_dict['Cars_ER']\n",
    "\n",
    "print(f\"Config: {config}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80eb58f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cuda\n",
      "✅ GPU: NVIDIA GeForce RTX 5080\n",
      "✅ FP16 (Mixed Precision): True\n",
      "LM: distilbert\n",
      "Epochs: 10\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        self.task = 'Cars_ER'\n",
    "        self.batch_size = 32\n",
    "        self.max_len = 128\n",
    "        self.lr = 3e-5\n",
    "        self.n_epochs = 10\n",
    "        self.lm = 'distilbert'  # Faster than RoBERTa\n",
    "        self.da = None  # Data augmentation\n",
    "        self.dk = None  # Domain knowledge\n",
    "        self.summarize = False\n",
    "        self.alpha_aug = 0.8\n",
    "        self.run_id = 0\n",
    "        self.save_model = True\n",
    "        self.logdir = DITTO_RESULTS_DIR\n",
    "        # Force GPU usage\n",
    "        self.device = 'cuda'\n",
    "        self.fp16 = True  # Enable mixed precision for faster GPU training\n",
    "\n",
    "hp = HyperParams()\n",
    "\n",
    "# Verify GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU not available! This training requires a GPU.\")\n",
    "    \n",
    "print(f\"✅ Device: {hp.device}\")\n",
    "print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"✅ FP16 (Mixed Precision): {hp.fp16}\")\n",
    "print(f\"LM: {hp.lm}\")\n",
    "print(f\"Epochs: {hp.n_epochs}\")\n",
    "print(f\"Batch size: {hp.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a573877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset...\n",
      "Train dataset: 10788 examples\n",
      "\n",
      "Loading validation dataset...\n",
      "Validation dataset: 2311 examples\n",
      "\n",
      "Loading test dataset...\n",
      "Test dataset: 2313 examples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading train dataset...\")\n",
    "train_dataset = DittoDataset(config['trainset'], lm=hp.lm, max_len=hp.max_len, da=hp.da)\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "\n",
    "print(\"\\nLoading validation dataset...\")\n",
    "valid_dataset = DittoDataset(config['validset'], lm=hp.lm, max_len=hp.max_len)\n",
    "print(f\"Validation dataset: {len(valid_dataset)} examples\")\n",
    "\n",
    "print(\"\\nLoading test dataset...\")\n",
    "test_dataset = DittoDataset(config['testset'], lm=hp.lm, max_len=hp.max_len)\n",
    "print(f\"Test dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef5c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING DITTO MODEL\n",
      "============================================================\n",
      "Running cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING DITTO MODEL\n",
      "============================================================\n",
      "Running cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1389.76it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\ditto_light\\ditto.py:144: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  scheduler.step()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING DITTO MODEL\n",
      "============================================================\n",
      "Running cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1389.76it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\ditto_light\\ditto.py:144: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  scheduler.step()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 0.657928466796875\n",
      "step: 10, loss: 0.5758132934570312\n",
      "step: 20, loss: 0.4860572814941406\n",
      "step: 30, loss: 0.13100528717041016\n",
      "step: 40, loss: 0.5732955932617188\n",
      "step: 50, loss: 0.03508177399635315\n",
      "step: 60, loss: 0.1079677939414978\n",
      "step: 70, loss: 0.021427392959594727\n",
      "step: 80, loss: 0.0054675862193107605\n",
      "step: 90, loss: 0.2797318398952484\n",
      "step: 100, loss: 0.10306283831596375\n",
      "step: 110, loss: 0.028403908014297485\n",
      "step: 120, loss: 0.20374122262001038\n",
      "step: 130, loss: 0.010962992906570435\n",
      "step: 140, loss: 0.011815749108791351\n",
      "step: 150, loss: 0.005495689809322357\n",
      "step: 160, loss: 0.005922652781009674\n",
      "step: 170, loss: 0.13485746085643768\n",
      "step: 180, loss: 0.0963040292263031\n",
      "step: 190, loss: 0.004587128758430481\n",
      "step: 200, loss: 0.010635815560817719\n",
      "step: 210, loss: 0.014300454407930374\n",
      "step: 220, loss: 0.01826491579413414\n",
      "step: 230, loss: 0.15486572682857513\n",
      "step: 240, loss: 0.01771315187215805\n",
      "step: 250, loss: 0.005220070481300354\n",
      "step: 260, loss: 0.20614036917686462\n",
      "step: 270, loss: 0.117469422519207\n",
      "step: 280, loss: 0.05198045074939728\n",
      "step: 290, loss: 0.010785415768623352\n",
      "step: 300, loss: 0.0020418688654899597\n",
      "step: 310, loss: 0.005322787910699844\n",
      "step: 320, loss: 0.04110809415578842\n",
      "step: 330, loss: 0.023857668042182922\n",
      "epoch 1: dev_f1=0.982648401826484, f1=0.9733447979363714, best_f1=0.9733447979363714\n",
      "step: 0, loss: 0.00594262033700943\n",
      "step: 10, loss: 0.015759218484163284\n",
      "step: 20, loss: 0.13002978265285492\n",
      "step: 30, loss: 0.0010590516030788422\n",
      "step: 40, loss: 0.0028777271509170532\n",
      "step: 50, loss: 0.05628573149442673\n",
      "step: 60, loss: 0.015915118157863617\n",
      "step: 70, loss: 0.00448356568813324\n",
      "step: 80, loss: 0.03499098867177963\n",
      "step: 90, loss: 0.0025112293660640717\n",
      "step: 100, loss: 0.12229491770267487\n",
      "step: 110, loss: 0.11862016469240189\n",
      "step: 120, loss: 0.06340847164392471\n",
      "step: 130, loss: 0.0007364638149738312\n",
      "step: 140, loss: 0.06290504336357117\n",
      "step: 150, loss: 0.011145427823066711\n",
      "step: 160, loss: 0.007082782685756683\n",
      "step: 170, loss: 0.05042105168104172\n",
      "step: 180, loss: 0.05738598853349686\n",
      "step: 190, loss: 0.00754522904753685\n",
      "step: 200, loss: 0.04288472235202789\n",
      "step: 210, loss: 0.10589198023080826\n",
      "step: 220, loss: 0.030665330588817596\n",
      "step: 230, loss: 0.0085158571600914\n",
      "step: 240, loss: 0.06531389057636261\n",
      "step: 250, loss: 0.2281186878681183\n",
      "step: 260, loss: 0.007883362472057343\n",
      "step: 270, loss: 0.015554480254650116\n",
      "step: 280, loss: 0.006506461650133133\n",
      "step: 290, loss: 0.07586352527141571\n",
      "step: 300, loss: 0.05109512805938721\n",
      "step: 310, loss: 0.007373690605163574\n",
      "step: 320, loss: 0.001697450876235962\n",
      "step: 330, loss: 0.058601632714271545\n",
      "epoch 2: dev_f1=0.9838129496402878, f1=0.9863945578231292, best_f1=0.9863945578231292\n",
      "step: 0, loss: 0.003798834979534149\n",
      "step: 10, loss: 0.011822719126939774\n",
      "step: 20, loss: 0.02822575345635414\n",
      "step: 30, loss: 0.0046849437057971954\n",
      "step: 40, loss: 0.06961439549922943\n",
      "step: 50, loss: 0.004378601908683777\n",
      "step: 60, loss: 0.033393971621990204\n",
      "step: 70, loss: 0.0010384507477283478\n",
      "step: 80, loss: 0.023025482892990112\n",
      "step: 90, loss: 0.0106315016746521\n",
      "step: 100, loss: 0.03412104770541191\n",
      "step: 110, loss: 0.07919758558273315\n",
      "step: 120, loss: 0.001616988331079483\n",
      "step: 130, loss: 0.0007173120975494385\n",
      "step: 140, loss: 0.013146955519914627\n",
      "step: 150, loss: 0.0014180690050125122\n",
      "step: 160, loss: 0.0017440207302570343\n",
      "step: 170, loss: 0.01033996045589447\n",
      "step: 180, loss: 0.062294114381074905\n",
      "step: 190, loss: 0.05854637175798416\n",
      "step: 200, loss: 0.0014770030975341797\n",
      "step: 210, loss: 0.00036491453647613525\n",
      "step: 220, loss: 0.08599154651165009\n",
      "step: 230, loss: 0.024317912757396698\n",
      "step: 240, loss: 0.053587593138217926\n",
      "step: 250, loss: 0.010969065129756927\n",
      "step: 260, loss: 0.0008845813572406769\n",
      "step: 270, loss: 0.000660281628370285\n",
      "step: 280, loss: 0.029110997915267944\n",
      "step: 290, loss: 0.18058329820632935\n",
      "step: 300, loss: 0.007536835968494415\n",
      "step: 310, loss: 0.02714795619249344\n",
      "step: 320, loss: 0.00030340254306793213\n",
      "step: 330, loss: 0.0032198503613471985\n",
      "epoch 3: dev_f1=0.9811659192825112, f1=0.9838022165387894, best_f1=0.9863945578231292\n",
      "step: 0, loss: 0.001381073147058487\n",
      "step: 10, loss: 0.0003237389028072357\n",
      "step: 20, loss: 0.00037183240056037903\n",
      "step: 30, loss: 0.00014477968215942383\n",
      "step: 40, loss: 0.08613139390945435\n",
      "step: 50, loss: 0.008225765079259872\n",
      "step: 60, loss: 0.0018654651939868927\n",
      "step: 70, loss: 0.0034101009368896484\n",
      "step: 80, loss: 0.00042348727583885193\n",
      "step: 90, loss: 0.0032284967601299286\n",
      "step: 100, loss: 0.0009309574961662292\n",
      "step: 110, loss: 0.08838775753974915\n",
      "step: 120, loss: 0.059613607823848724\n",
      "step: 130, loss: 0.002445407211780548\n",
      "step: 140, loss: 0.0015110932290554047\n",
      "step: 150, loss: 0.05447697639465332\n",
      "step: 160, loss: 0.0944860577583313\n",
      "step: 170, loss: 0.0003551580011844635\n",
      "step: 180, loss: 0.025155238807201385\n",
      "step: 190, loss: 0.0007428266108036041\n",
      "step: 200, loss: 0.04765203222632408\n",
      "step: 210, loss: 0.0012356117367744446\n",
      "step: 220, loss: 0.001025702804327011\n",
      "step: 230, loss: 0.00031680241227149963\n",
      "step: 240, loss: 0.0002936534583568573\n",
      "step: 250, loss: 0.0019569694995880127\n",
      "step: 260, loss: 0.003576453775167465\n",
      "step: 270, loss: 0.0007159560918807983\n",
      "step: 280, loss: 0.0003117285668849945\n",
      "step: 290, loss: 0.00041611865162849426\n",
      "step: 300, loss: 0.006765730679035187\n",
      "step: 310, loss: 0.00016872957348823547\n",
      "step: 320, loss: 0.09925447404384613\n",
      "step: 330, loss: 0.0007966607809066772\n",
      "epoch 4: dev_f1=0.9837837837837838, f1=0.9836909871244636, best_f1=0.9863945578231292\n",
      "step: 0, loss: 0.0018266253173351288\n",
      "step: 10, loss: 0.001145392656326294\n",
      "step: 20, loss: 0.008665449917316437\n",
      "step: 30, loss: 0.00632147490978241\n",
      "step: 40, loss: 0.005436573177576065\n",
      "step: 50, loss: 0.00020796805620193481\n",
      "step: 60, loss: 0.0027308762073516846\n",
      "step: 70, loss: 0.0003240145742893219\n",
      "step: 80, loss: 0.002026885747909546\n",
      "step: 90, loss: 0.0007373951375484467\n",
      "step: 100, loss: 0.0002681128680706024\n",
      "step: 110, loss: 0.0287286639213562\n",
      "step: 120, loss: 0.0044638365507125854\n",
      "step: 130, loss: 0.06974663585424423\n",
      "step: 140, loss: 0.1660681813955307\n",
      "step: 150, loss: 0.01094602420926094\n",
      "step: 160, loss: 0.0381959043443203\n",
      "step: 170, loss: 0.002575572580099106\n",
      "step: 180, loss: 0.00013052672147750854\n",
      "step: 190, loss: 0.00021123141050338745\n",
      "step: 200, loss: 0.0025958754122257233\n",
      "step: 210, loss: 0.04653620719909668\n",
      "step: 220, loss: 0.033201783895492554\n",
      "step: 230, loss: 0.01702062413096428\n",
      "step: 240, loss: 0.0006103776395320892\n",
      "step: 250, loss: 0.007583748549222946\n",
      "step: 260, loss: 0.0010414011776447296\n",
      "step: 270, loss: 0.00021191313862800598\n",
      "step: 280, loss: 0.068278968334198\n",
      "step: 290, loss: 0.0007571764290332794\n",
      "step: 300, loss: 0.0006781332194805145\n",
      "step: 310, loss: 0.0005859285593032837\n",
      "step: 320, loss: 0.00028178468346595764\n",
      "step: 330, loss: 0.0024122074246406555\n",
      "epoch 5: dev_f1=0.9838129496402878, f1=0.985531914893617, best_f1=0.9863945578231292\n",
      "step: 0, loss: 0.0037704147398471832\n",
      "step: 10, loss: 0.1150587797164917\n",
      "step: 20, loss: 0.004759997129440308\n",
      "step: 30, loss: 0.004342652857303619\n",
      "step: 40, loss: 0.010974172502756119\n",
      "step: 50, loss: 0.09415388852357864\n",
      "step: 60, loss: 0.007236789911985397\n",
      "step: 70, loss: 0.013165842741727829\n",
      "step: 80, loss: 0.011313047260046005\n",
      "step: 90, loss: 0.00011928007006645203\n",
      "step: 100, loss: 0.00026830658316612244\n",
      "step: 110, loss: 0.00045425817370414734\n",
      "step: 120, loss: 0.00027354806661605835\n",
      "step: 130, loss: 0.003839019685983658\n",
      "step: 140, loss: 0.00024046748876571655\n",
      "step: 150, loss: 5.358085036277771e-05\n",
      "step: 160, loss: 0.0002696327865123749\n",
      "step: 170, loss: 0.00017312169075012207\n",
      "step: 180, loss: 0.01127622276544571\n",
      "step: 190, loss: 0.0005529671907424927\n",
      "step: 200, loss: 0.0007417239248752594\n",
      "step: 210, loss: 0.00011438131332397461\n",
      "step: 220, loss: 0.006751261651515961\n",
      "step: 230, loss: 0.030709020793437958\n",
      "step: 240, loss: 0.006244588643312454\n",
      "step: 250, loss: 0.03483504056930542\n",
      "step: 260, loss: 0.012007426470518112\n",
      "step: 270, loss: 0.01232784241437912\n",
      "step: 280, loss: 0.040963321924209595\n",
      "step: 290, loss: 0.002568572759628296\n",
      "step: 300, loss: 0.00026780739426612854\n",
      "step: 310, loss: 0.002377014607191086\n",
      "step: 320, loss: 0.00017576292157173157\n",
      "step: 330, loss: 0.00025177001953125\n",
      "epoch 6: dev_f1=0.9838420107719928, f1=0.983857264231096, best_f1=0.983857264231096\n",
      "step: 0, loss: 0.00010962411761283875\n",
      "step: 10, loss: 0.0004126541316509247\n",
      "step: 20, loss: 0.11074817925691605\n",
      "step: 30, loss: 0.002394001930952072\n",
      "step: 40, loss: 0.0004351586103439331\n",
      "step: 50, loss: 0.02049693837761879\n",
      "step: 60, loss: 0.002187378704547882\n",
      "step: 70, loss: 0.006798900663852692\n",
      "step: 80, loss: 0.051491595804691315\n",
      "step: 90, loss: 0.01399560272693634\n",
      "step: 100, loss: 0.0016816891729831696\n",
      "step: 110, loss: 0.0001946091651916504\n",
      "step: 120, loss: 0.0010652504861354828\n",
      "step: 130, loss: 7.120147347450256e-05\n",
      "step: 140, loss: 0.004294596612453461\n",
      "step: 150, loss: 0.0022691376507282257\n",
      "step: 160, loss: 0.00013375654816627502\n",
      "step: 170, loss: 0.000674910843372345\n",
      "step: 180, loss: 7.560476660728455e-05\n",
      "step: 190, loss: 0.00026541948318481445\n",
      "step: 200, loss: 0.00013712793588638306\n",
      "step: 210, loss: 0.00011588633060455322\n",
      "step: 220, loss: 0.051653310656547546\n",
      "step: 230, loss: 0.00023677200078964233\n",
      "step: 240, loss: 0.0025917626917362213\n",
      "step: 250, loss: 0.0008688122034072876\n",
      "step: 260, loss: 0.005879975855350494\n",
      "step: 270, loss: 0.0004831254482269287\n",
      "step: 280, loss: 0.000655829906463623\n",
      "step: 290, loss: 0.0002627931535243988\n",
      "step: 300, loss: 0.02778005599975586\n",
      "step: 310, loss: 8.340179920196533e-05\n",
      "step: 320, loss: 0.00042812153697013855\n",
      "step: 330, loss: 0.0005476921796798706\n",
      "epoch 7: dev_f1=0.9855072463768116, f1=0.9837467921300257, best_f1=0.9837467921300257\n",
      "step: 0, loss: 0.006721802055835724\n",
      "step: 10, loss: 0.04004138335585594\n",
      "step: 20, loss: 0.031525082886219025\n",
      "step: 30, loss: 0.00013350322842597961\n",
      "step: 40, loss: 8.094683289527893e-05\n",
      "step: 50, loss: 0.00027431175112724304\n",
      "step: 60, loss: 0.004973679780960083\n",
      "step: 70, loss: 9.898841381072998e-05\n",
      "step: 80, loss: 0.000253237783908844\n",
      "step: 90, loss: 6.785616278648376e-05\n",
      "step: 100, loss: 0.00042207539081573486\n",
      "step: 110, loss: 0.0009025819599628448\n",
      "step: 120, loss: 0.00033688172698020935\n",
      "step: 130, loss: 0.0050516389310359955\n",
      "step: 140, loss: 0.00010109692811965942\n",
      "step: 150, loss: 0.01864207163453102\n",
      "step: 160, loss: 0.00042401254177093506\n",
      "step: 170, loss: 0.011951837688684464\n",
      "step: 180, loss: 0.00015423446893692017\n",
      "step: 190, loss: 0.00012445077300071716\n",
      "step: 200, loss: 0.0004806630313396454\n",
      "step: 210, loss: 8.614733815193176e-05\n",
      "step: 220, loss: 0.0007147938013076782\n",
      "step: 230, loss: 9.981170296669006e-05\n",
      "step: 240, loss: 8.162856101989746e-05\n",
      "step: 250, loss: 7.332116365432739e-05\n",
      "step: 260, loss: 0.009542863816022873\n",
      "step: 270, loss: 0.00016742944717407227\n",
      "step: 280, loss: 0.020199645310640335\n",
      "step: 290, loss: 8.809566497802734e-05\n",
      "step: 300, loss: 6.303191184997559e-05\n",
      "step: 310, loss: 0.00014158710837364197\n",
      "step: 320, loss: 0.009736418724060059\n",
      "step: 330, loss: 0.004655048251152039\n",
      "epoch 8: dev_f1=0.9836956521739131, f1=0.9828473413379074, best_f1=0.9837467921300257\n",
      "step: 0, loss: 0.006376162171363831\n",
      "step: 10, loss: 0.00015106797218322754\n",
      "step: 20, loss: 6.662309169769287e-05\n",
      "step: 30, loss: 4.348158836364746e-05\n",
      "step: 40, loss: 5.201995372772217e-05\n",
      "step: 50, loss: 0.00018025189638137817\n",
      "step: 60, loss: 0.02991117164492607\n",
      "step: 70, loss: 0.0002193152904510498\n",
      "step: 80, loss: 0.00018624961376190186\n",
      "step: 90, loss: 0.00016461685299873352\n",
      "step: 100, loss: 0.0015971139073371887\n",
      "step: 110, loss: 0.0008993931114673615\n",
      "step: 120, loss: 0.00011741742491722107\n",
      "step: 130, loss: 0.002894364297389984\n",
      "step: 140, loss: 0.00018818676471710205\n",
      "step: 150, loss: 0.00011343881487846375\n",
      "step: 160, loss: 0.025069501250982285\n",
      "step: 170, loss: 9.051337838172913e-05\n",
      "step: 180, loss: 5.89713454246521e-05\n",
      "step: 190, loss: 6.509199738502502e-05\n",
      "step: 200, loss: 5.8278441429138184e-05\n",
      "step: 210, loss: 8.885189890861511e-05\n",
      "step: 220, loss: 7.552653551101685e-05\n",
      "step: 230, loss: 4.063546657562256e-05\n",
      "step: 240, loss: 7.216259837150574e-05\n",
      "step: 250, loss: 0.00043934211134910583\n",
      "step: 260, loss: 4.5336782932281494e-05\n",
      "step: 270, loss: 0.0004286840558052063\n",
      "step: 280, loss: 4.2047351598739624e-05\n",
      "step: 290, loss: 0.0014001131057739258\n",
      "step: 300, loss: 0.0005903728306293488\n",
      "step: 310, loss: 6.143003702163696e-05\n",
      "step: 320, loss: 0.001368507742881775\n",
      "step: 330, loss: 0.00017087161540985107\n",
      "epoch 9: dev_f1=0.9836956521739131, f1=0.9836629406706793, best_f1=0.9837467921300257\n",
      "step: 0, loss: 0.00023010745644569397\n",
      "step: 10, loss: 5.307421088218689e-05\n",
      "step: 20, loss: 0.0031138285994529724\n",
      "step: 30, loss: 5.415081977844238e-05\n",
      "step: 40, loss: 0.0013234838843345642\n",
      "step: 50, loss: 0.00029735267162323\n",
      "step: 60, loss: 0.0007140487432479858\n",
      "step: 70, loss: 6.432831287384033e-05\n",
      "step: 80, loss: 8.869916200637817e-05\n",
      "step: 90, loss: 0.0005953013896942139\n",
      "step: 100, loss: 0.0014384761452674866\n",
      "step: 110, loss: 0.00012763962149620056\n",
      "step: 120, loss: 0.00035190582275390625\n",
      "step: 130, loss: 7.97957181930542e-05\n",
      "step: 140, loss: 4.500150680541992e-05\n",
      "step: 150, loss: 5.988031625747681e-05\n",
      "step: 160, loss: 9.50060784816742e-05\n",
      "step: 170, loss: 5.22807240486145e-05\n",
      "step: 180, loss: 5.111470818519592e-05\n",
      "step: 190, loss: 6.271153688430786e-05\n",
      "step: 200, loss: 0.0013813748955726624\n",
      "step: 210, loss: 0.00010822340846061707\n",
      "step: 220, loss: 0.0005789734423160553\n",
      "step: 230, loss: 0.003645561635494232\n",
      "step: 240, loss: 0.0003535561263561249\n",
      "step: 250, loss: 4.7754496335983276e-05\n",
      "step: 260, loss: 4.015117883682251e-05\n",
      "step: 270, loss: 4.447624087333679e-05\n",
      "step: 280, loss: 7.461756467819214e-05\n",
      "step: 290, loss: 0.00152553990483284\n",
      "step: 300, loss: 5.389377474784851e-05\n",
      "step: 310, loss: 7.739290595054626e-05\n",
      "step: 320, loss: 5.080178380012512e-05\n",
      "step: 330, loss: 7.236748933792114e-05\n",
      "epoch 10: dev_f1=0.9836956521739131, f1=0.9810344827586207, best_f1=0.9837467921300257\n",
      "\n",
      "Training completed in 84.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DITTO MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "run_tag = f\"{hp.task}_lm={hp.lm}_epochs={hp.n_epochs}\"\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "ditto_train(train_dataset, valid_dataset, test_dataset, run_tag, hp)\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d439ad1",
   "metadata": {},
   "source": [
    "## 6. Load Trained Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5196e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found trained model: c:\\Users\\migli\\HW6ID\\ditto_results\\Cars_ER\\model.pt\n"
     ]
    }
   ],
   "source": [
    "# Find the saved model\n",
    "model_dir = os.path.join(DITTO_RESULTS_DIR, hp.task)  # Model saved under task name\n",
    "model_path = os.path.join(model_dir, 'model.pt')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Found trained model: {model_path}\")\n",
    "else:\n",
    "    # Try alternative path with run_tag\n",
    "    model_dir = os.path.join(DITTO_RESULTS_DIR, run_tag.replace('/', '_'))\n",
    "    model_path = os.path.join(model_dir, 'model.pt')\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Found trained model: {model_path}\")\n",
    "    else:\n",
    "        # List what we have\n",
    "        print(f\"Looking for model in: {DITTO_RESULTS_DIR}\")\n",
    "        for root, dirs, files in os.walk(DITTO_RESULTS_DIR):\n",
    "            for f in files:\n",
    "                if f.endswith('.pt'):\n",
    "                    model_path = os.path.join(root, f)\n",
    "                    print(f\"Found: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b52d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 956.83it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 956.83it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertModel LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "vocab_layer_norm.weight | UNEXPECTED |  | \n",
      "vocab_projector.bias    | UNEXPECTED |  | \n",
      "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
      "vocab_transform.bias    | UNEXPECTED |  | \n",
      "vocab_transform.weight  | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the model for inference\n",
    "from torch.utils import data\n",
    "\n",
    "def load_model_for_inference(model_path, device, lm='distilbert'):\n",
    "    \"\"\"Load trained Ditto model for inference.\"\"\"\n",
    "    model = DittoModel(device=device, lm=lm)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Check if it's a full checkpoint or just state_dict\n",
    "    if isinstance(checkpoint, dict) and 'model' in checkpoint:\n",
    "        # Full checkpoint format\n",
    "        state_dict = checkpoint['model']\n",
    "    else:\n",
    "        # Direct state_dict\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = load_model_for_inference(model_path, hp.device, hp.lm)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1fd31",
   "metadata": {},
   "source": [
    "## 7. Prepare Blocking Candidates for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ab8c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 candidates: 964002 pairs\n",
      "B2 candidates: 77087 pairs\n"
     ]
    }
   ],
   "source": [
    "# Load blocking candidates\n",
    "B1_path = os.path.join(BLOCKING_RESULTS_DIR, 'candidates_B1.csv')\n",
    "B2_path = os.path.join(BLOCKING_RESULTS_DIR, 'candidates_B2.csv')\n",
    "\n",
    "B1_candidates = pd.read_csv(B1_path)\n",
    "B2_candidates = pd.read_csv(B2_path)\n",
    "\n",
    "print(f\"B1 candidates: {len(B1_candidates)} pairs\")\n",
    "print(f\"B2 candidates: {len(B2_candidates)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80ecf2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 15412 matches\n",
      "Ground truth set size: 15412\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth for evaluation\n",
    "ground_truth = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'ground_truth_complete.csv'))\n",
    "print(f\"Ground truth: {len(ground_truth)} matches\")\n",
    "\n",
    "# Create ground truth set using craigslist_id and used_cars_id\n",
    "# These match the vehicle_id in the original datasets which are used as id_A and id_B in blocking\n",
    "gt_set = set(zip(ground_truth['craigslist_id'].astype(str), ground_truth['used_cars_id'].astype(str)))\n",
    "print(f\"Ground truth set size: {len(gt_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67783a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined\n"
     ]
    }
   ],
   "source": [
    "# Function to create Ditto-format pairs from blocking candidates\n",
    "def prepare_blocking_for_ditto(candidates_df, dataset_A, dataset_B):\n",
    "    \"\"\"\n",
    "    Create Ditto-format pairs from blocking candidates.\n",
    "    \"\"\"\n",
    "    # Ensure IDs are strings for matching\n",
    "    dataset_A = dataset_A.copy()\n",
    "    dataset_B = dataset_B.copy()\n",
    "    \n",
    "    # Get ID column names - use vehicle_id for our datasets\n",
    "    A_id_col = 'vehicle_id'\n",
    "    B_id_col = 'vehicle_id'\n",
    "    \n",
    "    dataset_A[A_id_col] = dataset_A[A_id_col].astype(str)\n",
    "    dataset_B[B_id_col] = dataset_B[B_id_col].astype(str)\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    A_dict = dataset_A.set_index(A_id_col).to_dict('index')\n",
    "    B_dict = dataset_B.set_index(B_id_col).to_dict('index')\n",
    "    \n",
    "    lines = []\n",
    "    pair_ids = []\n",
    "    \n",
    "    # Get candidate ID columns - blocking files use id_A and id_B\n",
    "    cand_A_col = 'id_A'\n",
    "    cand_B_col = 'id_B'\n",
    "    \n",
    "    for _, row in tqdm(candidates_df.iterrows(), total=len(candidates_df), desc=\"Preparing candidates\"):\n",
    "        id_A = str(row[cand_A_col])\n",
    "        id_B = str(row[cand_B_col])\n",
    "        \n",
    "        if id_A not in A_dict or id_B not in B_dict:\n",
    "            continue\n",
    "        \n",
    "        rec_A = A_dict[id_A]\n",
    "        rec_B = B_dict[id_B]\n",
    "        \n",
    "        # Serialize records\n",
    "        tokens_A = []\n",
    "        tokens_B = []\n",
    "        \n",
    "        # Map A (Craigslist) attributes\n",
    "        A_mapping = {\n",
    "            'year': 'year',\n",
    "            'manufacturer': 'manufacturer',\n",
    "            'model': 'model',\n",
    "            'price': 'price',\n",
    "            'mileage': 'mileage',\n",
    "            'fuel_type': 'fuel',\n",
    "            'transmission': 'transmission'\n",
    "        }\n",
    "        \n",
    "        for attr, ditto_attr in A_mapping.items():\n",
    "            if attr in rec_A and pd.notna(rec_A[attr]):\n",
    "                val = str(rec_A[attr]).strip()\n",
    "                if val:\n",
    "                    tokens_A.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "        \n",
    "        # Map B (Used Cars) attributes - same structure\n",
    "        B_mapping = {\n",
    "            'year': 'year',\n",
    "            'manufacturer': 'manufacturer',\n",
    "            'model': 'model',\n",
    "            'price': 'price',\n",
    "            'mileage': 'mileage',\n",
    "            'fuel_type': 'fuel',\n",
    "            'transmission': 'transmission'\n",
    "        }\n",
    "        \n",
    "        for attr, ditto_attr in B_mapping.items():\n",
    "            if attr in rec_B and pd.notna(rec_B[attr]):\n",
    "                val = str(rec_B[attr]).strip()\n",
    "                if val:\n",
    "                    tokens_B.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "        \n",
    "        rec1_str = ' '.join(tokens_A)\n",
    "        rec2_str = ' '.join(tokens_B)\n",
    "        \n",
    "        # Use dummy label 0, will be replaced with prediction\n",
    "        line = f\"{rec1_str}\\t{rec2_str}\\t0\"\n",
    "        lines.append(line)\n",
    "        pair_ids.append((id_A, id_B))\n",
    "    \n",
    "    return lines, pair_ids\n",
    "\n",
    "print(\"Function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ea38a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location']\n",
      "Dataset B columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location']\n"
     ]
    }
   ],
   "source": [
    "# Reload datasets A and B from original\n",
    "os.chdir(WORKSPACE)\n",
    "dataset_A = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_A_no_vin.csv'))\n",
    "dataset_B = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_B_no_vin.csv'))\n",
    "os.chdir(DITTO_DIR)\n",
    "\n",
    "print(f\"Dataset A columns: {list(dataset_A.columns)[:10]}\")\n",
    "print(f\"Dataset B columns: {list(dataset_B.columns)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22a0bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing B1 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 964002/964002 [00:14<00:00, 64875.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 964002 pairs prepared\n",
      "\n",
      "Preparing B2 candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 77087/77087 [00:01<00:00, 64223.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2: 77087 pairs prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare B1 candidates\n",
    "print(\"Preparing B1 candidates...\")\n",
    "B1_lines, B1_pair_ids = prepare_blocking_for_ditto(B1_candidates, dataset_A, dataset_B)\n",
    "print(f\"B1: {len(B1_lines)} pairs prepared\")\n",
    "\n",
    "print(\"\\nPreparing B2 candidates...\")\n",
    "B2_lines, B2_pair_ids = prepare_blocking_for_ditto(B2_candidates, dataset_A, dataset_B)\n",
    "print(f\"B2: {len(B2_lines)} pairs prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950f558",
   "metadata": {},
   "source": [
    "## 8. Run Inference and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09edbf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "def predict_with_ditto(model, lines, batch_size=256, lm='distilbert', max_len=128, device='cuda'):\n",
    "    \"\"\"\n",
    "    Run Ditto model on a list of serialized pairs.\n",
    "    Returns probabilities for class 1 (match).\n",
    "    \"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Create dataset from lines\n",
    "    dataset = DittoDataset(lines, lm=lm, max_len=max_len)\n",
    "    \n",
    "    # Create dataloader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=dataset.pad, num_workers=0)\n",
    "    \n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "            if len(batch) == 2:\n",
    "                x, y = batch\n",
    "                if device == 'cuda':\n",
    "                    x = x.cuda()\n",
    "                logits = model(x)\n",
    "            else:\n",
    "                x1, x2, y = batch\n",
    "                if device == 'cuda':\n",
    "                    x1 = x1.cuda()\n",
    "                    x2 = x2.cuda()\n",
    "                logits = model(x1, x2)\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # Get probability of class 1 (match)\n",
    "            match_probs = probs[:, 1].cpu().numpy()\n",
    "            all_probs.extend(match_probs)\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "print(\"Prediction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53b6b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(predictions, pair_ids, gt_set, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \"\"\"\n",
    "    # Create predictions based on threshold\n",
    "    pred_labels = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    # Get true labels from ground truth\n",
    "    true_labels = [1 if (id_A, id_B) in gt_set else 0 for id_A, id_B in pair_ids]\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(true_labels, pred_labels, zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels, labels=[0, 1]).ravel()\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'total_positives_gt': int(sum(true_labels)),\n",
    "        'predicted_positives': int(sum(pred_labels))\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4749cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING B1-Ditto\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 3766/3766 [06:44<00:00,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B1 Inference time: 406.29 seconds\n",
      "B1 Predictions: 964002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference on B1\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING B1-Ditto\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B1_inference_start = time.time()\n",
    "B1_probs = predict_with_ditto(model, B1_lines, batch_size=256, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "B1_inference_time = time.time() - B1_inference_start\n",
    "\n",
    "print(f\"\\nB1 Inference time: {B1_inference_time:.2f} seconds\")\n",
    "print(f\"B1 Predictions: {len(B1_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a087df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B1 @ threshold=0.3:\n",
      "  Precision: 0.0103\n",
      "  Recall: 0.9271\n",
      "  F1: 0.0204\n",
      "  TP: 2186, FP: 210150, FN: 172, TN: 751494\n",
      "\n",
      "B1 @ threshold=0.5:\n",
      "  Precision: 0.0127\n",
      "  Recall: 0.9075\n",
      "  F1: 0.0250\n",
      "  TP: 2140, FP: 166686, FN: 218, TN: 794958\n",
      "\n",
      "B1 @ threshold=0.7:\n",
      "  Precision: 0.0158\n",
      "  Recall: 0.8838\n",
      "  F1: 0.0311\n",
      "  TP: 2084, FP: 129638, FN: 274, TN: 832006\n"
     ]
    }
   ],
   "source": [
    "# Evaluate B1 with multiple thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "B1_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    metrics = evaluate_predictions(B1_probs, B1_pair_ids, gt_set, threshold=thresh)\n",
    "    B1_results.append(metrics)\n",
    "    print(f\"\\nB1 @ threshold={thresh}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  TP: {metrics['tp']}, FP: {metrics['fp']}, FN: {metrics['fn']}, TN: {metrics['tn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "347ba693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING B2-Ditto\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Predicting: 100%|██████████| 302/302 [00:33<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B2 Inference time: 34.33 seconds\n",
      "B2 Predictions: 77087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference on B2\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING B2-Ditto\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B2_inference_start = time.time()\n",
    "B2_probs = predict_with_ditto(model, B2_lines, batch_size=256, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "B2_inference_time = time.time() - B2_inference_start\n",
    "\n",
    "print(f\"\\nB2 Inference time: {B2_inference_time:.2f} seconds\")\n",
    "print(f\"B2 Predictions: {len(B2_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "607f5873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B2 @ threshold=0.3:\n",
      "  Precision: 0.0576\n",
      "  Recall: 0.9424\n",
      "  F1: 0.1086\n",
      "  TP: 2177, FP: 35600, FN: 133, TN: 39177\n",
      "\n",
      "B2 @ threshold=0.5:\n",
      "  Precision: 0.0676\n",
      "  Recall: 0.9238\n",
      "  F1: 0.1259\n",
      "  TP: 2134, FP: 29454, FN: 176, TN: 45323\n",
      "\n",
      "B2 @ threshold=0.7:\n",
      "  Precision: 0.0804\n",
      "  Recall: 0.9009\n",
      "  F1: 0.1476\n",
      "  TP: 2081, FP: 23813, FN: 229, TN: 50964\n"
     ]
    }
   ],
   "source": [
    "# Evaluate B2 with multiple thresholds\n",
    "B2_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    metrics = evaluate_predictions(B2_probs, B2_pair_ids, gt_set, threshold=thresh)\n",
    "    B2_results.append(metrics)\n",
    "    print(f\"\\nB2 @ threshold={thresh}:\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  TP: {metrics['tp']}, FP: {metrics['fp']}, FN: {metrics['fn']}, TN: {metrics['tn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "563b771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 probability distribution:\n",
      "  Min: 0.0000\n",
      "  Max: 0.9998\n",
      "  Mean: 0.1891\n",
      "  Median: 0.0098\n",
      "  > 0.5: 168826\n",
      "  > 0.9: 84415\n",
      "  > 0.99: 29862\n",
      "\n",
      "B2 probability distribution:\n",
      "  Min: 0.0001\n",
      "  Max: 0.9998\n",
      "  Mean: 0.4197\n",
      "  Median: 0.2785\n",
      "  > 0.5: 31588\n",
      "  > 0.9: 18149\n",
      "  > 0.99: 8272\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check probability distribution\n",
    "print(\"B1 probability distribution:\")\n",
    "print(f\"  Min: {B1_probs.min():.4f}\")\n",
    "print(f\"  Max: {B1_probs.max():.4f}\")\n",
    "print(f\"  Mean: {B1_probs.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(B1_probs):.4f}\")\n",
    "print(f\"  > 0.5: {(B1_probs > 0.5).sum()}\")\n",
    "print(f\"  > 0.9: {(B1_probs > 0.9).sum()}\")\n",
    "print(f\"  > 0.99: {(B1_probs > 0.99).sum()}\")\n",
    "\n",
    "print(\"\\nB2 probability distribution:\")\n",
    "print(f\"  Min: {B2_probs.min():.4f}\")\n",
    "print(f\"  Max: {B2_probs.max():.4f}\")\n",
    "print(f\"  Mean: {B2_probs.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(B2_probs):.4f}\")\n",
    "print(f\"  > 0.5: {(B2_probs > 0.5).sum()}\")\n",
    "print(f\"  > 0.9: {(B2_probs > 0.9).sum()}\")\n",
    "print(f\"  > 0.99: {(B2_probs > 0.99).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bbb80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training data distribution...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check training data distribution - are labels balanced?\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChecking training data distribution...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mtrain_path\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     train_labels = [\u001b[38;5;28mint\u001b[39m(line.strip().split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain labels - 0 (non-match): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels.count(\u001b[32m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 1 (match): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels.count(\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Check training data distribution - are labels balanced?\n",
    "print(\"Checking training data distribution...\")\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_labels = [int(line.strip().split('\\t')[-1]) for line in f]\n",
    "    \n",
    "print(f\"Train labels - 0 (non-match): {train_labels.count(0)}, 1 (match): {train_labels.count(1)}\")\n",
    "\n",
    "with open(valid_path, 'r', encoding='utf-8') as f:\n",
    "    valid_labels = [int(line.strip().split('\\t')[-1]) for line in f]\n",
    "print(f\"Valid labels - 0 (non-match): {valid_labels.count(0)}, 1 (match): {valid_labels.count(1)}\")\n",
    "\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_labels = [int(line.strip().split('\\t')[-1]) for line in f]\n",
    "print(f\"Test labels - 0 (non-match): {test_labels.count(0)}, 1 (match): {test_labels.count(1)}\")\n",
    "\n",
    "# Also check a few samples\n",
    "print(\"\\n--- Sample training lines ---\")\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        parts = line.strip().split('\\t')\n",
    "        print(f\"Label: {parts[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbfdcb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 10/10 [00:01<00:00,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Predictions:\n",
      "  Min prob: 0.0000\n",
      "  Max prob: 0.9999\n",
      "  Mean prob: 0.2512\n",
      "  Median prob: 0.0000\n",
      "\n",
      "For true NON-MATCH (label=0): mean pred = 0.0000\n",
      "For true MATCH (label=1): mean pred = 0.9999\n",
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Non-Match       1.00      1.00      1.00      1732\n",
      "       Match       1.00      1.00      1.00       581\n",
      "\n",
      "    accuracy                           1.00      2313\n",
      "   macro avg       1.00      1.00      1.00      2313\n",
      "weighted avg       1.00      1.00      1.00      2313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test model on test set to see if it's working\n",
    "print(\"Testing model on test set...\")\n",
    "\n",
    "# Load test data\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_all_lines = [line.strip() for line in f]\n",
    "\n",
    "# Get predictions\n",
    "test_preds = predict_with_ditto(model, test_all_lines, batch_size=256, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "\n",
    "# Get true labels\n",
    "test_true = [int(line.split('\\t')[-1]) for line in test_all_lines]\n",
    "\n",
    "print(f\"\\nTest Set Predictions:\")\n",
    "print(f\"  Min prob: {test_preds.min():.4f}\")\n",
    "print(f\"  Max prob: {test_preds.max():.4f}\")\n",
    "print(f\"  Mean prob: {test_preds.mean():.4f}\")\n",
    "print(f\"  Median prob: {np.median(test_preds):.4f}\")\n",
    "\n",
    "# Distribution by true label\n",
    "test_true = np.array(test_true)\n",
    "print(f\"\\nFor true NON-MATCH (label=0): mean pred = {test_preds[test_true == 0].mean():.4f}\")\n",
    "print(f\"For true MATCH (label=1): mean pred = {test_preds[test_true == 1].mean():.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "from sklearn.metrics import classification_report\n",
    "test_pred_labels = (test_preds >= 0.5).astype(int)\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(test_true, test_pred_labels, target_names=['Non-Match', 'Match']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c2a7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample TEST line (working correctly) ===\n",
      "COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\tCOL manufacturer VAL Chevrolet COL model VAL Equinox COL price VAL 23934.0 COL mileage VAL 5.0\t0\n",
      "\n",
      "=== Sample B1 lines (not working) ===\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0 COL fuel VAL gas COL transmission VAL automatic\tCOL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0 COL fuel VAL gasoline COL transmission VAL automatic\t0\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0 COL fuel VAL gas COL transmission VAL automatic\tCOL year VAL 2012 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 7999.0 COL mileage VAL 106803.0 COL fuel VAL gasoline COL transmission VAL 5-speed automatic overdrive\t0\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0 COL fuel VAL gas COL transmission VAL automatic\tCOL year VAL 2009 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 6500.0 COL mileage VAL 73340.0 COL fuel VAL gasoline COL transmission VAL 5-speed automatic\t0\n",
      "\n",
      "=== Checking if B1_lines look correct ===\n",
      "Number of TAB separators in test line: 2\n",
      "Number of TAB separators in B1 line: 2\n"
     ]
    }
   ],
   "source": [
    "# Check sample B1 lines vs sample test lines\n",
    "print(\"=== Sample TEST line (working correctly) ===\")\n",
    "print(test_all_lines[0])\n",
    "print()\n",
    "\n",
    "print(\"=== Sample B1 lines (not working) ===\")\n",
    "for i in range(3):\n",
    "    print(B1_lines[i])\n",
    "print()\n",
    "\n",
    "# Check if B1_lines have the right format\n",
    "print(\"=== Checking if B1_lines look correct ===\")\n",
    "print(f\"Number of TAB separators in test line: {test_all_lines[0].count(chr(9))}\")\n",
    "print(f\"Number of TAB separators in B1 line: {B1_lines[0].count(chr(9))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8064808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed function defined\n"
     ]
    }
   ],
   "source": [
    "# FIXED: Recreate B1/B2 lines with the SAME format as training data\n",
    "# Training uses: manufacturer, model, price, mileage (NO year, fuel, transmission)\n",
    "\n",
    "def prepare_blocking_for_ditto_fixed(candidates_df, dataset_A, dataset_B):\n",
    "    \"\"\"\n",
    "    Create Ditto-format pairs from blocking candidates.\n",
    "    Using SAME format as training data.\n",
    "    \"\"\"\n",
    "    dataset_A = dataset_A.copy()\n",
    "    dataset_B = dataset_B.copy()\n",
    "    \n",
    "    A_id_col = 'vehicle_id'\n",
    "    B_id_col = 'vehicle_id'\n",
    "    \n",
    "    dataset_A[A_id_col] = dataset_A[A_id_col].astype(str)\n",
    "    dataset_B[B_id_col] = dataset_B[B_id_col].astype(str)\n",
    "    \n",
    "    A_dict = dataset_A.set_index(A_id_col).to_dict('index')\n",
    "    B_dict = dataset_B.set_index(B_id_col).to_dict('index')\n",
    "    \n",
    "    lines = []\n",
    "    pair_ids = []\n",
    "    \n",
    "    cand_A_col = 'id_A'\n",
    "    cand_B_col = 'id_B'\n",
    "    \n",
    "    # Use SAME attributes as training: manufacturer, model, price, mileage\n",
    "    # NOTE: Training used year, manufacturer, model, price, mileage\n",
    "    attr_mapping = [\n",
    "        ('year', 'year'),\n",
    "        ('manufacturer', 'manufacturer'),\n",
    "        ('model', 'model'),\n",
    "        ('price', 'price'),\n",
    "        ('mileage', 'mileage'),\n",
    "    ]\n",
    "    \n",
    "    for _, row in tqdm(candidates_df.iterrows(), total=len(candidates_df), desc=\"Preparing candidates\"):\n",
    "        id_A = str(row[cand_A_col])\n",
    "        id_B = str(row[cand_B_col])\n",
    "        \n",
    "        if id_A not in A_dict or id_B not in B_dict:\n",
    "            continue\n",
    "        \n",
    "        rec_A = A_dict[id_A]\n",
    "        rec_B = B_dict[id_B]\n",
    "        \n",
    "        tokens_A = []\n",
    "        tokens_B = []\n",
    "        \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_A and pd.notna(rec_A[data_attr]):\n",
    "                val = str(rec_A[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_A.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "                    \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_B and pd.notna(rec_B[data_attr]):\n",
    "                val = str(rec_B[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_B.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "        \n",
    "        rec1_str = ' '.join(tokens_A)\n",
    "        rec2_str = ' '.join(tokens_B)\n",
    "        \n",
    "        # Dummy label\n",
    "        line = f\"{rec1_str}\\t{rec2_str}\\t0\"\n",
    "        lines.append(line)\n",
    "        pair_ids.append((id_A, id_B))\n",
    "    \n",
    "    return lines, pair_ids\n",
    "\n",
    "print(\"Fixed function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ea40e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location', 'longitude', 'manufacturer', 'mileage', 'model', 'price', 'source_dataset', 'transmission', 'vehicle_id', 'year']\n",
      "Dataset B columns: ['body_type', 'cylinders', 'description', 'drive_type', 'exterior_color', 'fuel_type', 'image_url', 'latitude', 'listing_date', 'location', 'longitude', 'manufacturer', 'mileage', 'model', 'price', 'source_dataset', 'transmission', 'vehicle_id', 'year']\n",
      "B1 candidates: 964002\n",
      "B2 candidates: 77087\n"
     ]
    }
   ],
   "source": [
    "# Reload datasets and regenerate B1/B2 lines with fixed format\n",
    "os.chdir(WORKSPACE)\n",
    "dataset_A = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_A_no_vin.csv'))\n",
    "dataset_B = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_B_no_vin.csv'))\n",
    "\n",
    "# Reload blocking candidates\n",
    "B1_candidates = pd.read_csv(os.path.join(BLOCKING_RESULTS_DIR, 'candidates_B1.csv'))\n",
    "B2_candidates = pd.read_csv(os.path.join(BLOCKING_RESULTS_DIR, 'candidates_B2.csv'))\n",
    "\n",
    "print(f\"Dataset A columns: {list(dataset_A.columns)}\")\n",
    "print(f\"Dataset B columns: {list(dataset_B.columns)}\")\n",
    "print(f\"B1 candidates: {len(B1_candidates)}\")\n",
    "print(f\"B2 candidates: {len(B2_candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aad81e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating B1 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 964002/964002 [00:13<00:00, 71812.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 964002 pairs\n",
      "\n",
      "Regenereating B2 lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 77087/77087 [00:01<00:00, 72485.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2: 77087 pairs\n",
      "\n",
      "=== OLD B1 line ===\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0 COL fuel VAL gas COL transmission VAL automatic\tCOL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0 COL fuel VAL gasoline COL transmission VAL automatic\t0\n",
      "\n",
      "=== NEW B1 line (fixed) ===\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\tCOL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\t0\n",
      "\n",
      "=== Sample TEST line ===\n",
      "COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\tCOL manufacturer VAL Chevrolet COL model VAL Equinox COL price VAL 23934.0 COL mileage VAL 5.0\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Regenerate B1 and B2 lines with fixed format\n",
    "print(\"Regenerating B1 lines...\")\n",
    "B1_lines_fixed, B1_pair_ids_fixed = prepare_blocking_for_ditto_fixed(B1_candidates, dataset_A, dataset_B)\n",
    "print(f\"B1: {len(B1_lines_fixed)} pairs\")\n",
    "\n",
    "print(\"\\nRegenereating B2 lines...\")\n",
    "B2_lines_fixed, B2_pair_ids_fixed = prepare_blocking_for_ditto_fixed(B2_candidates, dataset_A, dataset_B)\n",
    "print(f\"B2: {len(B2_lines_fixed)} pairs\")\n",
    "\n",
    "# Compare old vs new format\n",
    "print(\"\\n=== OLD B1 line ===\")\n",
    "print(B1_lines[0])\n",
    "print(\"\\n=== NEW B1 line (fixed) ===\")\n",
    "print(B1_lines_fixed[0])\n",
    "print(\"\\n=== Sample TEST line ===\")\n",
    "print(test_all_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3539c9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating B1 lines with EXACT training format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 964002/964002 [00:12<00:00, 76350.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 964002 pairs\n",
      "\n",
      "Regenerating B2 lines with EXACT training format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 77087/77087 [00:01<00:00, 75696.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2: 77087 pairs\n",
      "\n",
      "=== NEW B1 line (final) ===\n",
      "COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\tCOL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\t0\n",
      "\n",
      "=== Sample TEST line (training format) ===\n",
      "COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\tCOL manufacturer VAL Chevrolet COL model VAL Equinox COL price VAL 23934.0 COL mileage VAL 5.0\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FINAL FIX: Match EXACTLY training format - NO year, just manufacturer, model, price, mileage\n",
    "\n",
    "def prepare_blocking_for_ditto_final(candidates_df, dataset_A, dataset_B):\n",
    "    \"\"\"\n",
    "    Create Ditto-format pairs from blocking candidates.\n",
    "    Using EXACT same format as training data: manufacturer, model, price, mileage\n",
    "    \"\"\"\n",
    "    dataset_A = dataset_A.copy()\n",
    "    dataset_B = dataset_B.copy()\n",
    "    \n",
    "    A_id_col = 'vehicle_id'\n",
    "    B_id_col = 'vehicle_id'\n",
    "    \n",
    "    dataset_A[A_id_col] = dataset_A[A_id_col].astype(str)\n",
    "    dataset_B[B_id_col] = dataset_B[B_id_col].astype(str)\n",
    "    \n",
    "    A_dict = dataset_A.set_index(A_id_col).to_dict('index')\n",
    "    B_dict = dataset_B.set_index(B_id_col).to_dict('index')\n",
    "    \n",
    "    lines = []\n",
    "    pair_ids = []\n",
    "    \n",
    "    cand_A_col = 'id_A'\n",
    "    cand_B_col = 'id_B'\n",
    "    \n",
    "    # EXACT same attributes as training: manufacturer, model, price, mileage (NO year!)\n",
    "    attr_mapping = [\n",
    "        ('manufacturer', 'manufacturer'),\n",
    "        ('model', 'model'),\n",
    "        ('price', 'price'),\n",
    "        ('mileage', 'mileage'),\n",
    "    ]\n",
    "    \n",
    "    for _, row in tqdm(candidates_df.iterrows(), total=len(candidates_df), desc=\"Preparing candidates\"):\n",
    "        id_A = str(row[cand_A_col])\n",
    "        id_B = str(row[cand_B_col])\n",
    "        \n",
    "        if id_A not in A_dict or id_B not in B_dict:\n",
    "            continue\n",
    "        \n",
    "        rec_A = A_dict[id_A]\n",
    "        rec_B = B_dict[id_B]\n",
    "        \n",
    "        tokens_A = []\n",
    "        tokens_B = []\n",
    "        \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_A and pd.notna(rec_A[data_attr]):\n",
    "                val = str(rec_A[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_A.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "                    \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_B and pd.notna(rec_B[data_attr]):\n",
    "                val = str(rec_B[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_B.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "        \n",
    "        rec1_str = ' '.join(tokens_A)\n",
    "        rec2_str = ' '.join(tokens_B)\n",
    "        \n",
    "        # Dummy label\n",
    "        line = f\"{rec1_str}\\t{rec2_str}\\t0\"\n",
    "        lines.append(line)\n",
    "        pair_ids.append((id_A, id_B))\n",
    "    \n",
    "    return lines, pair_ids\n",
    "\n",
    "# Regenerate\n",
    "print(\"Regenerating B1 lines with EXACT training format...\")\n",
    "B1_lines_final, B1_pair_ids_final = prepare_blocking_for_ditto_final(B1_candidates, dataset_A, dataset_B)\n",
    "print(f\"B1: {len(B1_lines_final)} pairs\")\n",
    "\n",
    "print(\"\\nRegenerating B2 lines with EXACT training format...\")\n",
    "B2_lines_final, B2_pair_ids_final = prepare_blocking_for_ditto_final(B2_candidates, dataset_A, dataset_B)\n",
    "print(f\"B2: {len(B2_lines_final)} pairs\")\n",
    "\n",
    "# Verify format matches\n",
    "print(\"\\n=== NEW B1 line (final) ===\")\n",
    "print(B1_lines_final[0])\n",
    "print(\"\\n=== Sample TEST line (training format) ===\")\n",
    "print(test_all_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92502909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RE-RUNNING INFERENCE WITH CORRECTED FORMAT\n",
      "============================================================\n",
      "\n",
      "B1 Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 3766/3766 [04:14<00:00, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 Inference time: 255.72 seconds\n",
      "B1 Predictions: 964002\n",
      "\n",
      "B1 Probability Distribution:\n",
      "  Min: 0.0000\n",
      "  Max: 0.0059\n",
      "  Mean: 0.0007\n",
      "  Median: 0.0002\n",
      "  > 0.5: 0\n",
      "  > 0.9: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-run inference with corrected format\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RE-RUNNING INFERENCE WITH CORRECTED FORMAT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# B1 Inference\n",
    "print(\"\\nB1 Inference...\")\n",
    "B1_start = time.time()\n",
    "B1_probs_corrected = predict_with_ditto(model, B1_lines_final, batch_size=256, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "B1_time = time.time() - B1_start\n",
    "print(f\"B1 Inference time: {B1_time:.2f} seconds\")\n",
    "print(f\"B1 Predictions: {len(B1_probs_corrected)}\")\n",
    "\n",
    "# Check distribution\n",
    "print(f\"\\nB1 Probability Distribution:\")\n",
    "print(f\"  Min: {B1_probs_corrected.min():.4f}\")\n",
    "print(f\"  Max: {B1_probs_corrected.max():.4f}\")\n",
    "print(f\"  Mean: {B1_probs_corrected.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(B1_probs_corrected):.4f}\")\n",
    "print(f\"  > 0.5: {(B1_probs_corrected > 0.5).sum()}\")\n",
    "print(f\"  > 0.9: {(B1_probs_corrected > 0.9).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6a8709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample POSITIVE training pair (label=1) ===\n",
      "COL year VAL 2013.0 COL manufacturer VAL Kia COL model VAL Soul + COL price VAL 8995 COL mileage VAL 105873.0\tCOL year VAL 2013.0 COL manufacturer VAL Kia COL model VAL Soul COL price VAL 9995.0 COL mileage VAL 105237.0\t1\n",
      "\n",
      "=== Sample from B1 (with similar manufacturer) ===\n",
      "COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\tCOL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\t0\n",
      "\n",
      "=== B1 predictions stats ===\n",
      "Number of predictions > 0.3: 0\n",
      "Number of predictions > 0.1: 0\n",
      "Number of predictions > 0.01: 0\n",
      "Number of predictions > 0.001: 221172\n"
     ]
    }
   ],
   "source": [
    "# Let's check if there's an issue with how training data was created\n",
    "# Compare training positive pairs vs B1 pairs\n",
    "\n",
    "print(\"=== Sample POSITIVE training pair (label=1) ===\")\n",
    "for line in test_all_lines:\n",
    "    if line.endswith('\\t1'):\n",
    "        print(line[:500])\n",
    "        break\n",
    "\n",
    "print(\"\\n=== Sample from B1 (with similar manufacturer) ===\")\n",
    "for line in B1_lines_final[:100]:\n",
    "    if 'Mazda' in line:\n",
    "        print(line[:500])\n",
    "        break\n",
    "\n",
    "# Check if there are any actual matches in B1 by looking at probabilities\n",
    "print(\"\\n=== B1 predictions stats ===\")\n",
    "print(f\"Number of predictions > 0.3: {(B1_probs_corrected > 0.3).sum()}\")\n",
    "print(f\"Number of predictions > 0.1: {(B1_probs_corrected > 0.1).sum()}\")\n",
    "print(f\"Number of predictions > 0.01: {(B1_probs_corrected > 0.01).sum()}\")\n",
    "print(f\"Number of predictions > 0.001: {(B1_probs_corrected > 0.001).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed617bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regenerating B1 lines WITH year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 964002/964002 [00:13<00:00, 72492.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1: 964002 pairs\n",
      "\n",
      "Regenerating B2 lines WITH year...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing candidates: 100%|██████████| 77087/77087 [00:01<00:00, 72109.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B2: 77087 pairs\n",
      "\n",
      "=== Training positive sample ===\n",
      "COL year VAL 2013.0 COL manufacturer VAL Kia COL model VAL Soul + COL price VAL 8995 COL mileage VAL 105873.0\tCOL year VAL 2013.0 COL manufacturer VAL Kia COL model VAL Soul COL price VAL 9995.0 COL mileage VAL 105237.0\t1\n",
      "\n",
      "=== New B1 line (v3 with year) ===\n",
      "COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\tCOL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The training data has YEAR! Let me fix to use year\n",
    "# Training format: COL year VAL ... COL manufacturer VAL ... COL model VAL ... COL price VAL ... COL mileage VAL ...\n",
    "\n",
    "def prepare_blocking_for_ditto_with_year(candidates_df, dataset_A, dataset_B):\n",
    "    \"\"\"\n",
    "    Create Ditto-format pairs from blocking candidates.\n",
    "    Using EXACT same format as training data: year, manufacturer, model, price, mileage\n",
    "    \"\"\"\n",
    "    dataset_A = dataset_A.copy()\n",
    "    dataset_B = dataset_B.copy()\n",
    "    \n",
    "    A_id_col = 'vehicle_id'\n",
    "    B_id_col = 'vehicle_id'\n",
    "    \n",
    "    dataset_A[A_id_col] = dataset_A[A_id_col].astype(str)\n",
    "    dataset_B[B_id_col] = dataset_B[B_id_col].astype(str)\n",
    "    \n",
    "    A_dict = dataset_A.set_index(A_id_col).to_dict('index')\n",
    "    B_dict = dataset_B.set_index(B_id_col).to_dict('index')\n",
    "    \n",
    "    lines = []\n",
    "    pair_ids = []\n",
    "    \n",
    "    cand_A_col = 'id_A'\n",
    "    cand_B_col = 'id_B'\n",
    "    \n",
    "    # EXACT same attributes as training: year, manufacturer, model, price, mileage\n",
    "    attr_mapping = [\n",
    "        ('year', 'year'),\n",
    "        ('manufacturer', 'manufacturer'),\n",
    "        ('model', 'model'),\n",
    "        ('price', 'price'),\n",
    "        ('mileage', 'mileage'),\n",
    "    ]\n",
    "    \n",
    "    for _, row in tqdm(candidates_df.iterrows(), total=len(candidates_df), desc=\"Preparing candidates\"):\n",
    "        id_A = str(row[cand_A_col])\n",
    "        id_B = str(row[cand_B_col])\n",
    "        \n",
    "        if id_A not in A_dict or id_B not in B_dict:\n",
    "            continue\n",
    "        \n",
    "        rec_A = A_dict[id_A]\n",
    "        rec_B = B_dict[id_B]\n",
    "        \n",
    "        tokens_A = []\n",
    "        tokens_B = []\n",
    "        \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_A and pd.notna(rec_A[data_attr]):\n",
    "                val = str(rec_A[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_A.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "                    \n",
    "        for ditto_attr, data_attr in attr_mapping:\n",
    "            if data_attr in rec_B and pd.notna(rec_B[data_attr]):\n",
    "                val = str(rec_B[data_attr]).strip()\n",
    "                if val:\n",
    "                    tokens_B.append(f\"COL {ditto_attr} VAL {val}\")\n",
    "        \n",
    "        rec1_str = ' '.join(tokens_A)\n",
    "        rec2_str = ' '.join(tokens_B)\n",
    "        \n",
    "        line = f\"{rec1_str}\\t{rec2_str}\\t0\"\n",
    "        lines.append(line)\n",
    "        pair_ids.append((id_A, id_B))\n",
    "    \n",
    "    return lines, pair_ids\n",
    "\n",
    "# Regenerate WITH year\n",
    "print(\"Regenerating B1 lines WITH year...\")\n",
    "B1_lines_v3, B1_pair_ids_v3 = prepare_blocking_for_ditto_with_year(B1_candidates, dataset_A, dataset_B)\n",
    "print(f\"B1: {len(B1_lines_v3)} pairs\")\n",
    "\n",
    "print(\"\\nRegenerating B2 lines WITH year...\")\n",
    "B2_lines_v3, B2_pair_ids_v3 = prepare_blocking_for_ditto_with_year(B2_candidates, dataset_A, dataset_B)\n",
    "print(f\"B2: {len(B2_lines_v3)} pairs\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n=== Training positive sample ===\")\n",
    "print(test_all_lines[next(i for i,l in enumerate(test_all_lines) if l.endswith('\\t1'))])\n",
    "print(\"\\n=== New B1 line (v3 with year) ===\")\n",
    "print(B1_lines_v3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0182976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "B1 INFERENCE (with year)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 3766/3766 [05:09<00:00, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B1 Inference time: 310.61 seconds\n",
      "\n",
      "B1 Probability Distribution:\n",
      "  Min: 0.9991\n",
      "  Max: 0.9999\n",
      "  Mean: 0.9999\n",
      "  Median: 0.9999\n",
      "  > 0.5: 964002\n",
      "  > 0.3: 964002\n",
      "  > 0.1: 964002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference with v3 (with year)\n",
    "print(\"=\"*60)\n",
    "print(\"B1 INFERENCE (with year)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "B1_start_v3 = time.time()\n",
    "B1_probs_v3 = predict_with_ditto(model, B1_lines_v3, batch_size=256, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "B1_time_v3 = time.time() - B1_start_v3\n",
    "print(f\"B1 Inference time: {B1_time_v3:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nB1 Probability Distribution:\")\n",
    "print(f\"  Min: {B1_probs_v3.min():.4f}\")\n",
    "print(f\"  Max: {B1_probs_v3.max():.4f}\")\n",
    "print(f\"  Mean: {B1_probs_v3.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(B1_probs_v3):.4f}\")\n",
    "print(f\"  > 0.5: {(B1_probs_v3 > 0.5).sum()}\")\n",
    "print(f\"  > 0.3: {(B1_probs_v3 > 0.3).sum()}\")\n",
    "print(f\"  > 0.1: {(B1_probs_v3 > 0.1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dec367fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model on known training examples...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_all_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m test_sample_labels = []\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get some negatives\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_all_lines\u001b[49m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m line.endswith(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_samples) < \u001b[32m5\u001b[39m:\n\u001b[32m     11\u001b[39m         test_samples.append(line)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_all_lines' is not defined"
     ]
    }
   ],
   "source": [
    "# Debug: Test the model on training examples directly to see what's happening\n",
    "\n",
    "# Get some positive and negative examples from the training data\n",
    "print(\"Testing model on known training examples...\")\n",
    "test_samples = []\n",
    "test_sample_labels = []\n",
    "\n",
    "# Get some negatives\n",
    "for line in test_all_lines:\n",
    "    if line.endswith('\\t0') and len(test_samples) < 5:\n",
    "        test_samples.append(line)\n",
    "        test_sample_labels.append(0)\n",
    "    elif line.endswith('\\t1') and len(test_samples) < 10:\n",
    "        test_samples.append(line)\n",
    "        test_sample_labels.append(1)\n",
    "    if len(test_samples) >= 10:\n",
    "        break\n",
    "\n",
    "# Predict\n",
    "sample_probs = predict_with_ditto(model, test_samples, batch_size=16, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "\n",
    "print(\"\\nPredictions on test samples:\")\n",
    "for i, (prob, label) in enumerate(zip(sample_probs, test_sample_labels)):\n",
    "    print(f\"  Sample {i+1}: True={label}, Pred prob={prob:.4f} ({'MATCH' if prob > 0.5 else 'NO MATCH'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "067d8f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test negative (correctly classified) ===\n",
      "'COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\\tCOL manufacturer VAL Chevrolet COL model VAL Equinox COL price VAL 23934.0 COL mileage VAL 5.0\\t0'\n",
      "\n",
      "=== B1 sample (incorrectly classified as match) ===\n",
      "'COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\\tCOL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\\t0'\n",
      "\n",
      "Test sample parts: 3\n",
      "Test left: COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\n",
      "Test right: COL manufacturer VAL Chevrolet COL model VAL Equinox COL price VAL 23934.0 COL mileage VAL 5.0\n",
      "\n",
      "B1 sample parts: 3\n",
      "B1 left: COL year VAL 2014.0 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 0 COL mileage VAL 60047.0\n",
      "B1 right: COL year VAL 2008 COL manufacturer VAL Mazda COL model VAL Mazda5 COL price VAL 4999.0 COL mileage VAL 130122.0\n"
     ]
    }
   ],
   "source": [
    "# Compare character by character what's different\n",
    "\n",
    "print(\"=== Test negative (correctly classified) ===\")\n",
    "test_neg = test_samples[0]\n",
    "print(repr(test_neg))\n",
    "print()\n",
    "\n",
    "print(\"=== B1 sample (incorrectly classified as match) ===\")\n",
    "b1_sample = B1_lines_v3[0]\n",
    "print(repr(b1_sample))\n",
    "print()\n",
    "\n",
    "# Check if there's a difference in tokenization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Parse test sample\n",
    "t_parts = test_neg.strip().split('\\t')\n",
    "print(f\"Test sample parts: {len(t_parts)}\")\n",
    "print(f\"Test left: {t_parts[0]}\")\n",
    "print(f\"Test right: {t_parts[1]}\")\n",
    "\n",
    "# Parse B1 sample\n",
    "b_parts = b1_sample.strip().split('\\t')\n",
    "print(f\"\\nB1 sample parts: {len(b_parts)}\")\n",
    "print(f\"B1 left: {b_parts[0]}\")\n",
    "print(f\"B1 right: {b_parts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb395592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Actual training file lines (first 5) ===\n",
      "1: COL manufacturer VAL Chevrolet COL model VAL Colorado Z71 4X4 Gas COL price VAL 39999 COL mileage VAL 13303.0\tCOL manufacturer VAL Chevrolet COL model...\n",
      "2: COL manufacturer VAL Chevrolet COL model VAL Suburban COL price VAL 23999 COL mileage VAL 200754.0\tCOL manufacturer VAL Mazda COL model VAL Mazda6 COL...\n",
      "3: COL manufacturer VAL Ford COL model VAL Explorer COL price VAL 0 COL mileage VAL 5507.0\tCOL manufacturer VAL Dodge COL model VAL Journey COL price VAL...\n",
      "4: COL manufacturer VAL Jeep COL model VAL Wrangler COL price VAL 19999 COL mileage VAL 81879.0\tCOL manufacturer VAL Gmc COL model VAL Acadia COL price V...\n",
      "5: COL year VAL 2016.0 COL manufacturer VAL Gmc COL model VAL Acadia COL price VAL 29999 COL mileage VAL 38264.0\tCOL year VAL 2016.0 COL manufacturer VAL...\n",
      "\n",
      "=== Sample test_all_lines (first 5) ===\n",
      "1: COL manufacturer VAL Nissan COL model VAL Maxima COL price VAL 3975 COL mileage VAL 147799.0\tCOL manufacturer VAL Chevrolet COL model VAL Equinox COL ...\n",
      "2: COL manufacturer VAL Ford COL model VAL Mustang COL price VAL 38800 COL mileage VAL 14171.0\tCOL manufacturer VAL Honda COL model VAL Pilot COL price V...\n",
      "3: COL year VAL 2013.0 COL manufacturer VAL Kia COL model VAL Soul + COL price VAL 8995 COL mileage VAL 105873.0\tCOL year VAL 2013.0 COL manufacturer VAL...\n",
      "4: COL manufacturer VAL Ford COL model VAL Explorer COL price VAL 0 COL mileage VAL 1001.0\tCOL manufacturer VAL Mazda COL model VAL Cx-5 COL price VAL 28...\n",
      "5: COL manufacturer VAL Nissan COL model VAL Cube COL price VAL 7495 COL mileage VAL 92060.0\tCOL manufacturer VAL Dodge COL model VAL Durango COL price V...\n"
     ]
    }
   ],
   "source": [
    "# Check the actual training file content\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_lines_actual = [f.readline().strip() for _ in range(5)]\n",
    "\n",
    "print(\"=== Actual training file lines (first 5) ===\")\n",
    "for i, line in enumerate(train_lines_actual):\n",
    "    print(f\"{i+1}: {line[:150]}...\")\n",
    "    \n",
    "print(\"\\n=== Sample test_all_lines (first 5) ===\")\n",
    "for i, line in enumerate(test_all_lines[:5]):\n",
    "    print(f\"{i+1}: {line[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c41382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing B1_lines_final (without year) on first 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 7/7 [00:00<00:00, 33.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B1 (no year) first 100 distribution:\n",
      "  Min: 0.0001\n",
      "  Max: 0.0030\n",
      "  Mean: 0.0011\n",
      "  > 0.5: 0\n",
      "\n",
      "Testing B1_lines_v3 (with year) on first 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 7/7 [00:00<00:00, 159.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B1 (with year) first 100 distribution:\n",
      "  Min: 0.9999\n",
      "  Max: 0.9999\n",
      "  Mean: 0.9999\n",
      "  > 0.5: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The issue might be simpler - let's check if B1 pairs are actually VERY similar\n",
    "# (same manufacturer/model), which might legitimately look like matches to the model\n",
    "\n",
    "# Let's check if B1 lines without year work better\n",
    "B1_no_year = []\n",
    "for line in B1_lines_v3[:100]:\n",
    "    parts = line.split('\\t')\n",
    "    # Remove year from each part\n",
    "    left = ' '.join([p for p in parts[0].split(' COL ') if not p.startswith('year')])\n",
    "    right = ' '.join([p for p in parts[1].split(' COL ') if not p.startswith('year')])\n",
    "    # Reconstruct with COL prefix\n",
    "    left_fixed = 'COL ' + left.strip() if not left.startswith('COL') else left\n",
    "    right_fixed = 'COL ' + right.strip() if not right.startswith('COL') else right\n",
    "    B1_no_year.append(f\"{left_fixed}\\t{right_fixed}\\t0\")\n",
    "\n",
    "# Actually - let me just use the version without year I already made\n",
    "print(\"Testing B1_lines_final (without year) on first 100...\")\n",
    "sample_probs_noyear = predict_with_ditto(model, B1_lines_final[:100], batch_size=16, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "\n",
    "print(f\"\\nB1 (no year) first 100 distribution:\")\n",
    "print(f\"  Min: {sample_probs_noyear.min():.4f}\")\n",
    "print(f\"  Max: {sample_probs_noyear.max():.4f}\")\n",
    "print(f\"  Mean: {sample_probs_noyear.mean():.4f}\")\n",
    "print(f\"  > 0.5: {(sample_probs_noyear > 0.5).sum()}\")\n",
    "\n",
    "# Compare with year version\n",
    "print(\"\\nTesting B1_lines_v3 (with year) on first 100...\")\n",
    "sample_probs_year = predict_with_ditto(model, B1_lines_v3[:100], batch_size=16, lm=hp.lm, max_len=hp.max_len, device=hp.device)\n",
    "\n",
    "print(f\"\\nB1 (with year) first 100 distribution:\")\n",
    "print(f\"  Min: {sample_probs_year.min():.4f}\")\n",
    "print(f\"  Max: {sample_probs_year.max():.4f}\")\n",
    "print(f\"  Mean: {sample_probs_year.mean():.4f}\")\n",
    "print(f\"  > 0.5: {(sample_probs_year > 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f10c4133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lines with year: 2724 (25.3%)\n",
      "Training lines without year: 8064 (74.7%)\n",
      "\n",
      "With year - label=1: 2724 (100.0%)\n",
      "Without year - label=1: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# The issue: training data has INCONSISTENT format (some with year, some without)\n",
    "# This caused the model to learn spurious correlations\n",
    "\n",
    "# Let's check the distribution in training data\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    all_train_lines = [l.strip() for l in f]\n",
    "    \n",
    "with_year = sum(1 for l in all_train_lines if 'COL year' in l)\n",
    "without_year = len(all_train_lines) - with_year\n",
    "\n",
    "print(f\"Training lines with year: {with_year} ({100*with_year/len(all_train_lines):.1f}%)\")\n",
    "print(f\"Training lines without year: {without_year} ({100*without_year/len(all_train_lines):.1f}%)\")\n",
    "\n",
    "# Check if there's a correlation between year presence and labels\n",
    "with_year_labels = [int(l.split('\\t')[-1]) for l in all_train_lines if 'COL year' in l]\n",
    "without_year_labels = [int(l.split('\\t')[-1]) for l in all_train_lines if 'COL year' not in l]\n",
    "\n",
    "print(f\"\\nWith year - label=1: {sum(with_year_labels)} ({100*sum(with_year_labels)/len(with_year_labels):.1f}%)\")\n",
    "print(f\"Without year - label=1: {sum(without_year_labels)} ({100*sum(without_year_labels)/len(without_year_labels):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23e21ce",
   "metadata": {},
   "source": [
    "## FIX: Ricreare i dati di training con formato consistente\n",
    "\n",
    "Abbiamo scoperto che il modello ha imparato una correlazione spuria:\n",
    "- Righe CON \"year\" → 100% label=1 (match)\n",
    "- Righe SENZA \"year\" → 100% label=0 (non-match)\n",
    "\n",
    "Questo è dovuto a un bug nella creazione dei dati. Dobbiamo:\n",
    "1. Ricreare i dati con formato consistente (sempre o mai year)\n",
    "2. Ri-addestrare il modello\n",
    "3. Ri-eseguire l'inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10caedf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting datasets with consistent format (always include year)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 10788/10788 [00:00<00:00, 44149.66it/s]\n",
      "Converting: 100%|██████████| 2311/2311 [00:00<00:00, 43604.78it/s]\n",
      "Converting: 100%|██████████| 2313/2313 [00:00<00:00, 44860.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10788 pairs\n",
      "Valid: 2311 pairs\n",
      "Test: 2313 pairs\n",
      "\n",
      "Train lines with year: 2724 / 10788\n",
      "\n",
      "=== Sample positive (label=1) ===\n",
      "COL year VAL 2016.0 COL manufacturer VAL Gmc COL model VAL Acadia COL price VAL 29999 COL mileage VAL 38264.0\tCOL year VAL 2016.0 COL manufacturer VAL Gmc COL model VAL Acadia COL price VAL 27499.0 CO\n",
      "\n",
      "=== Sample negative (label=0) ===\n",
      "COL manufacturer VAL Chevrolet COL model VAL Colorado Z71 4X4 Gas COL price VAL 39999 COL mileage VAL 13303.0\tCOL manufacturer VAL Chevrolet COL model VAL Cruze COL price VAL 19990.0 COL mileage VAL 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FIX: Recreate training/validation/test data with CONSISTENT format\n",
    "# Always include: year, manufacturer, model, price, mileage (when available)\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "train_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'validation.csv'))  # Fixed filename\n",
    "test_df = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'test.csv'))\n",
    "os.chdir(DITTO_DIR)\n",
    "\n",
    "def serialize_record_v2(row, prefix_cr=True):\n",
    "    \"\"\"\n",
    "    Serialize a record to Ditto format with CONSISTENT attributes.\n",
    "    Always includes: year, manufacturer, model, price, mileage\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    if prefix_cr:\n",
    "        # Craigslist record\n",
    "        attrs = [\n",
    "            ('year', 'year'),\n",
    "            ('manufacturer', 'manufacturer_cr'),\n",
    "            ('model', 'model_cr'),\n",
    "            ('price', 'price_cr'),\n",
    "            ('mileage', 'mileage_cr'),\n",
    "        ]\n",
    "    else:\n",
    "        # Used Cars record  \n",
    "        attrs = [\n",
    "            ('year', 'year'),\n",
    "            ('manufacturer', 'manufacturer_uc'),\n",
    "            ('model', 'model_uc'),\n",
    "            ('price', 'price_uc'),\n",
    "            ('mileage', 'mileage_uc'),\n",
    "        ]\n",
    "    \n",
    "    for attr_name, col_name in attrs:\n",
    "        if col_name in row.index:\n",
    "            val = row[col_name]\n",
    "            if pd.notna(val):\n",
    "                val_str = str(val).strip()\n",
    "                if val_str:\n",
    "                    tokens.append(f\"COL {attr_name} VAL {val_str}\")\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def convert_to_ditto_format_v2(df):\n",
    "    \"\"\"Convert to Ditto format with consistent attributes\"\"\"\n",
    "    lines = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Converting\"):\n",
    "        rec1 = serialize_record_v2(row, prefix_cr=True)\n",
    "        rec2 = serialize_record_v2(row, prefix_cr=False)\n",
    "        label = int(row['label'])\n",
    "        line = f\"{rec1}\\t{rec2}\\t{label}\"\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "print(\"Converting datasets with consistent format (always include year)...\")\n",
    "train_lines_v2 = convert_to_ditto_format_v2(train_df)\n",
    "valid_lines_v2 = convert_to_ditto_format_v2(valid_df)\n",
    "test_lines_v2 = convert_to_ditto_format_v2(test_df)\n",
    "\n",
    "print(f\"Train: {len(train_lines_v2)} pairs\")\n",
    "print(f\"Valid: {len(valid_lines_v2)} pairs\")\n",
    "print(f\"Test: {len(test_lines_v2)} pairs\")\n",
    "\n",
    "# Check for year consistency\n",
    "with_year_new = sum(1 for l in train_lines_v2 if 'COL year' in l)\n",
    "print(f\"\\nTrain lines with year: {with_year_new} / {len(train_lines_v2)}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\n=== Sample positive (label=1) ===\")\n",
    "pos = [l for l in train_lines_v2 if l.endswith('\\t1')][0]\n",
    "print(pos[:200])\n",
    "\n",
    "print(\"\\n=== Sample negative (label=0) ===\")\n",
    "neg = [l for l in train_lines_v2 if l.endswith('\\t0')][0]\n",
    "print(neg[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdecc9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved consistent training files:\n",
      "   c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\train.txt\n",
      "   c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\valid.txt\n",
      "   c:\\Users\\migli\\HW6ID\\FAIR-DA4ER\\ditto\\data\\cars\\test.txt\n",
      "\n",
      "✅ Verification - No spurious correlation:\n",
      "   With year (2724 lines): 2724 matches (100.0%)\n",
      "   Without year (8064 lines): 0 matches (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Save the new consistent training files\n",
    "train_path_v2 = os.path.join(DITTO_DATA_DIR, 'train.txt')\n",
    "valid_path_v2 = os.path.join(DITTO_DATA_DIR, 'valid.txt')\n",
    "test_path_v2 = os.path.join(DITTO_DATA_DIR, 'test.txt')\n",
    "\n",
    "with open(train_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_lines_v2))\n",
    "\n",
    "with open(valid_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(valid_lines_v2))\n",
    "\n",
    "with open(test_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test_lines_v2))\n",
    "\n",
    "print(f\"\\n✅ Saved consistent training files:\")\n",
    "print(f\"   {train_path_v2}\")\n",
    "print(f\"   {valid_path_v2}\")\n",
    "print(f\"   {test_path_v2}\")\n",
    "\n",
    "# Verify no spurious correlation\n",
    "with_year_new = sum(1 for l in train_lines_v2 if 'COL year' in l)\n",
    "with_year_labels = [int(l.split('\\t')[-1]) for l in train_lines_v2 if 'COL year' in l]\n",
    "without_year_labels = [int(l.split('\\t')[-1]) for l in train_lines_v2 if 'COL year' not in l]\n",
    "\n",
    "print(f\"\\n✅ Verification - No spurious correlation:\")\n",
    "print(f\"   With year ({with_year_new} lines): {sum(with_year_labels)} matches ({100*sum(with_year_labels)/len(with_year_labels):.1f}%)\")\n",
    "print(f\"   Without year ({len(train_lines_v2)-with_year_new} lines): {sum(without_year_labels)} matches ({100*sum(without_year_labels)/len(without_year_labels):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6748162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe columns:\n",
      "['vin', 'craigslist_id', 'used_cars_id', 'year', 'manufacturer_cr', 'manufacturer_uc', 'model_cr', 'model_uc', 'price_cr', 'price_uc', 'mileage_cr', 'mileage_uc', 'warnings', 'warning_details', 'label']\n",
      "\n",
      "Year column null values: 8064 / 10788\n",
      "\n",
      "Positive pairs: 2724 - year null: 0\n",
      "Negative pairs: 8064 - year null: 8064\n"
     ]
    }
   ],
   "source": [
    "# Check the training dataframe structure\n",
    "print(\"Training dataframe columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(f\"\\nYear column null values: {train_df['year'].isna().sum()} / {len(train_df)}\")\n",
    "\n",
    "# Check if positive pairs have year\n",
    "pos_df = train_df[train_df['label'] == 1]\n",
    "neg_df = train_df[train_df['label'] == 0]\n",
    "\n",
    "print(f\"\\nPositive pairs: {len(pos_df)} - year null: {pos_df['year'].isna().sum()}\")\n",
    "print(f\"Negative pairs: {len(neg_df)} - year null: {neg_df['year'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c16c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A columns with year:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check dataset_A and dataset_B for year columns\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset A columns with year:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m([c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset_A\u001b[49m.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c.lower()])\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset A year null: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_A[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m].isna().sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_A)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataset B columns with year:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset_A' is not defined"
     ]
    }
   ],
   "source": [
    "# Check dataset_A and dataset_B for year columns\n",
    "print(\"Dataset A columns with year:\")\n",
    "print([c for c in dataset_A.columns if 'year' in c.lower()])\n",
    "print(f\"Dataset A year null: {dataset_A['year'].isna().sum()} / {len(dataset_A)}\")\n",
    "\n",
    "print(\"\\nDataset B columns with year:\")\n",
    "print([c for c in dataset_B.columns if 'year' in c.lower()])\n",
    "print(f\"Dataset B year null: {dataset_B['year'].isna().sum()} / {len(dataset_B)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a60c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A: 14492 records\n",
      "Dataset A year null: 57 (0.4%)\n",
      "\n",
      "Dataset B: 15375 records\n",
      "Dataset B year null: 0 (0.0%)\n",
      "\n",
      "--- Checking if negative pairs could have year ---\n",
      "  Pair 0: A_year=2020.0, B_year=2019, train_year=nan\n",
      "  Pair 1: A_year=2015.0, B_year=2020, train_year=nan\n",
      "  Pair 2: A_year=2020.0, B_year=2020, train_year=nan\n",
      "  Pair 3: A_year=2014.0, B_year=2020, train_year=nan\n",
      "  Pair 5: A_year=2020.0, B_year=2021, train_year=nan\n"
     ]
    }
   ],
   "source": [
    "# Load datasets to check year availability\n",
    "os.chdir(WORKSPACE)\n",
    "dataset_A = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_A_no_vin.csv'))\n",
    "dataset_B = pd.read_csv(os.path.join(GROUND_TRUTH_DIR, 'dataset_B_no_vin.csv'))\n",
    "\n",
    "print(f\"Dataset A: {len(dataset_A)} records\")\n",
    "print(f\"Dataset A year null: {dataset_A['year'].isna().sum()} ({100*dataset_A['year'].isna().sum()/len(dataset_A):.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset B: {len(dataset_B)} records\")\n",
    "print(f\"Dataset B year null: {dataset_B['year'].isna().sum()} ({100*dataset_B['year'].isna().sum()/len(dataset_B):.1f}%)\")\n",
    "\n",
    "# Check if train negatives could have year\n",
    "print(\"\\n--- Checking if negative pairs could have year ---\")\n",
    "negative_samples = train_df[train_df['label'] == 0].head(10)\n",
    "for idx, row in negative_samples.iterrows():\n",
    "    cr_id = str(row['craigslist_id'])\n",
    "    uc_id = str(row['used_cars_id'])\n",
    "    \n",
    "    # Find in datasets\n",
    "    rec_A = dataset_A[dataset_A['vehicle_id'].astype(str) == cr_id]\n",
    "    rec_B = dataset_B[dataset_B['vehicle_id'].astype(str) == uc_id]\n",
    "    \n",
    "    if not rec_A.empty and not rec_B.empty:\n",
    "        year_A = rec_A.iloc[0]['year'] if pd.notna(rec_A.iloc[0]['year']) else 'NULL'\n",
    "        year_B = rec_B.iloc[0]['year'] if pd.notna(rec_B.iloc[0]['year']) else 'NULL'\n",
    "        print(f\"  Pair {idx}: A_year={year_A}, B_year={year_B}, train_year={row['year']}\")\n",
    "        if idx >= 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd434bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding train/valid/test with year from original datasets...\n",
      "\n",
      "✅ Train fixed: year null = 0 / 10788\n",
      "   Positives with year: 2724 / 2724\n",
      "   Negatives with year: 8064 / 8064\n",
      "\n",
      "✅ Valid fixed: year null = 0 / 2311\n",
      "✅ Test fixed: year null = 0 / 2313\n"
     ]
    }
   ],
   "source": [
    "# FIX: Rebuild train/valid/test dataframes with year from original datasets\n",
    "def rebuild_ground_truth_with_year(gt_df, dataset_A, dataset_B):\n",
    "    \"\"\"Rebuild ground truth with year from original datasets\"\"\"\n",
    "    gt_fixed = gt_df.copy()\n",
    "    \n",
    "    # Create lookup dictionaries\n",
    "    A_dict = dataset_A.set_index('vehicle_id').to_dict('index')\n",
    "    B_dict = dataset_B.set_index('vehicle_id').to_dict('index')\n",
    "    \n",
    "    # Fix year column\n",
    "    for idx, row in gt_fixed.iterrows():\n",
    "        cr_id = str(row['craigslist_id'])\n",
    "        uc_id = str(row['used_cars_id'])\n",
    "        \n",
    "        if cr_id in A_dict and uc_id in B_dict:\n",
    "            rec_A = A_dict[cr_id]\n",
    "            rec_B = B_dict[uc_id]\n",
    "            \n",
    "            # Use year from dataset_A (craigslist) or dataset_B if A is null\n",
    "            year_A = rec_A.get('year')\n",
    "            year_B = rec_B.get('year')\n",
    "            \n",
    "            if pd.notna(year_A):\n",
    "                gt_fixed.at[idx, 'year'] = year_A\n",
    "            elif pd.notna(year_B):\n",
    "                gt_fixed.at[idx, 'year'] = year_B\n",
    "    \n",
    "    return gt_fixed\n",
    "\n",
    "print(\"Rebuilding train/valid/test with year from original datasets...\")\n",
    "dataset_A['vehicle_id'] = dataset_A['vehicle_id'].astype(str)\n",
    "dataset_B['vehicle_id'] = dataset_B['vehicle_id'].astype(str)\n",
    "train_df['craigslist_id'] = train_df['craigslist_id'].astype(str)\n",
    "train_df['used_cars_id'] = train_df['used_cars_id'].astype(str)\n",
    "valid_df['craigslist_id'] = valid_df['craigslist_id'].astype(str)\n",
    "valid_df['used_cars_id'] = valid_df['used_cars_id'].astype(str)\n",
    "test_df['craigslist_id'] = test_df['craigslist_id'].astype(str)\n",
    "test_df['used_cars_id'] = test_df['used_cars_id'].astype(str)\n",
    "\n",
    "train_fixed = rebuild_ground_truth_with_year(train_df, dataset_A, dataset_B)\n",
    "valid_fixed = rebuild_ground_truth_with_year(valid_df, dataset_A, dataset_B)\n",
    "test_fixed = rebuild_ground_truth_with_year(test_df, dataset_A, dataset_B)\n",
    "\n",
    "print(f\"\\n✅ Train fixed: year null = {train_fixed['year'].isna().sum()} / {len(train_fixed)}\")\n",
    "print(f\"   Positives with year: {train_fixed[train_fixed['label']==1]['year'].notna().sum()} / {train_fixed['label'].sum()}\")\n",
    "print(f\"   Negatives with year: {train_fixed[train_fixed['label']==0]['year'].notna().sum()} / {(1-train_fixed['label']).sum()}\")\n",
    "\n",
    "print(f\"\\n✅ Valid fixed: year null = {valid_fixed['year'].isna().sum()} / {len(valid_fixed)}\")\n",
    "print(f\"✅ Test fixed: year null = {test_fixed['year'].isna().sum()} / {len(test_fixed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1404d505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting fixed dataframes to Ditto format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 10788/10788 [00:00<00:00, 43671.19it/s]\n",
      "Converting: 100%|██████████| 2311/2311 [00:00<00:00, 45271.50it/s]\n",
      "Converting: 100%|██████████| 2313/2313 [00:00<00:00, 44362.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved FIXED training files with year for all records\n",
      "\n",
      "✅ VERIFICATION - Spurious correlation ELIMINATED:\n",
      "   Lines with year: 10788 / 10788 (100.0%)\n",
      "     - Matches: 2724 (25.3%)\n",
      "     - Non-matches: 8064 (74.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert fixed dataframes to Ditto format\n",
    "print(\"Converting fixed dataframes to Ditto format...\")\n",
    "train_lines_fixed = convert_to_ditto_format_v2(train_fixed)\n",
    "valid_lines_fixed = convert_to_ditto_format_v2(valid_fixed)\n",
    "test_lines_fixed = convert_to_ditto_format_v2(test_fixed)\n",
    "\n",
    "# Save the fixed files\n",
    "with open(train_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_lines_fixed))\n",
    "\n",
    "with open(valid_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(valid_lines_fixed))\n",
    "\n",
    "with open(test_path_v2, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(test_lines_fixed))\n",
    "\n",
    "print(f\"\\n✅ Saved FIXED training files with year for all records\")\n",
    "\n",
    "# Verify NO spurious correlation now\n",
    "with_year_count = sum(1 for l in train_lines_fixed if 'COL year' in l)\n",
    "with_year_labels = [int(l.split('\\t')[-1]) for l in train_lines_fixed if 'COL year' in l]\n",
    "without_year_count = len(train_lines_fixed) - with_year_count\n",
    "\n",
    "print(f\"\\n✅ VERIFICATION - Spurious correlation ELIMINATED:\")\n",
    "print(f\"   Lines with year: {with_year_count} / {len(train_lines_fixed)} ({100*with_year_count/len(train_lines_fixed):.1f}%)\")\n",
    "if with_year_count > 0:\n",
    "    print(f\"     - Matches: {sum(with_year_labels)} ({100*sum(with_year_labels)/len(with_year_labels):.1f}%)\")\n",
    "    print(f\"     - Non-matches: {len(with_year_labels)-sum(with_year_labels)} ({100*(len(with_year_labels)-sum(with_year_labels))/len(with_year_labels):.1f}%)\")\n",
    "if without_year_count > 0:\n",
    "    without_year_labels = [int(l.split('\\t')[-1]) for l in train_lines_fixed if 'COL year' not in l]\n",
    "    print(f\"   Lines without year: {without_year_count}\")\n",
    "    print(f\"     - Matches: {sum(without_year_labels)} ({100*sum(without_year_labels)/len(without_year_labels):.1f}%)\")\n",
    "    print(f\"     - Non-matches: {len(without_year_labels)-sum(without_year_labels)} ({100*(len(without_year_labels)-sum(without_year_labels))/len(without_year_labels):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b57d5e",
   "metadata": {},
   "source": [
    "## 9. Summary and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ca0cb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DITTO MODEL - FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "Model: distilbert\n",
      "Training epochs: 10\n",
      "Training time: 84.07 seconds\n",
      "\n",
      "----------------------------------------\n",
      "B1-Ditto Results (threshold=0.5):\n",
      "----------------------------------------\n",
      "  Precision: 0.0127\n",
      "  Recall: 0.9075\n",
      "  F1: 0.0250\n",
      "  Inference time: 406.29 seconds\n",
      "\n",
      "----------------------------------------\n",
      "B2-Ditto Results (threshold=0.5):\n",
      "----------------------------------------\n",
      "  Precision: 0.0676\n",
      "  Recall: 0.9238\n",
      "  F1: 0.1259\n",
      "  Inference time: 34.33 seconds\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"DITTO MODEL - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModel: {hp.lm}\")\n",
    "print(f\"Training epochs: {hp.n_epochs}\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"B1-Ditto Results (threshold=0.5):\")\n",
    "print(\"-\"*40)\n",
    "b1_best = B1_results[1]  # threshold 0.5\n",
    "print(f\"  Precision: {b1_best['precision']:.4f}\")\n",
    "print(f\"  Recall: {b1_best['recall']:.4f}\")\n",
    "print(f\"  F1: {b1_best['f1']:.4f}\")\n",
    "print(f\"  Inference time: {B1_inference_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"B2-Ditto Results (threshold=0.5):\")\n",
    "print(\"-\"*40)\n",
    "b2_best = B2_results[1]  # threshold 0.5\n",
    "print(f\"  Precision: {b2_best['precision']:.4f}\")\n",
    "print(f\"  Recall: {b2_best['recall']:.4f}\")\n",
    "print(f\"  F1: {b2_best['f1']:.4f}\")\n",
    "print(f\"  Inference time: {B2_inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b530eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to c:\\Users\\migli\\HW6ID\\ditto_results\\ditto_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Save all results\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "# Convert numpy types to Python native types for JSON serialization\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Convert numpy types to Python native types\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # numpy scalar\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "results = {\n",
    "    'model': hp.lm,\n",
    "    'n_epochs': hp.n_epochs,\n",
    "    'batch_size': hp.batch_size,\n",
    "    'max_len': hp.max_len,\n",
    "    'learning_rate': hp.lr,\n",
    "    'device': hp.device,\n",
    "    'training_time_seconds': training_time,\n",
    "    'B1': {\n",
    "        'inference_time_seconds': B1_inference_time,\n",
    "        'num_candidates': len(B1_lines),\n",
    "        'results_by_threshold': convert_to_native(B1_results)\n",
    "    },\n",
    "    'B2': {\n",
    "        'inference_time_seconds': B2_inference_time,\n",
    "        'num_candidates': len(B2_lines),\n",
    "        'results_by_threshold': convert_to_native(B2_results)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "with open(os.path.join(DITTO_RESULTS_DIR, 'ditto_metrics.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results saved to {os.path.join(DITTO_RESULTS_DIR, 'ditto_metrics.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f230a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to:\n",
      "  c:\\Users\\migli\\HW6ID\\ditto_results\\ditto_predictions_B1.csv\n",
      "  c:\\Users\\migli\\HW6ID\\ditto_results\\ditto_predictions_B2.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "B1_pred_df = pd.DataFrame({\n",
    "    'craigslist_id': [p[0] for p in B1_pair_ids],\n",
    "    'used_cars_id': [p[1] for p in B1_pair_ids],\n",
    "    'probability': B1_probs,\n",
    "    'predicted_match_05': (B1_probs >= 0.5).astype(int),\n",
    "    'true_match': [1 if p in gt_set else 0 for p in B1_pair_ids]\n",
    "})\n",
    "B1_pred_df.to_csv(os.path.join(DITTO_RESULTS_DIR, 'ditto_predictions_B1.csv'), index=False)\n",
    "\n",
    "B2_pred_df = pd.DataFrame({\n",
    "    'craigslist_id': [p[0] for p in B2_pair_ids],\n",
    "    'used_cars_id': [p[1] for p in B2_pair_ids],\n",
    "    'probability': B2_probs,\n",
    "    'predicted_match_05': (B2_probs >= 0.5).astype(int),\n",
    "    'true_match': [1 if p in gt_set else 0 for p in B2_pair_ids]\n",
    "})\n",
    "B2_pred_df.to_csv(os.path.join(DITTO_RESULTS_DIR, 'ditto_predictions_B2.csv'), index=False)\n",
    "\n",
    "print(f\"Saved predictions to:\")\n",
    "print(f\"  {os.path.join(DITTO_RESULTS_DIR, 'ditto_predictions_B1.csv')}\")\n",
    "print(f\"  {os.path.join(DITTO_RESULTS_DIR, 'ditto_predictions_B2.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c23206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold Comparison:\n",
      "Blocking  Threshold  Precision   Recall       F1   TP     FP\n",
      "      B1        0.3   0.010295 0.927057 0.020364 2186 210150\n",
      "      B2        0.3   0.057628 0.942424 0.108614 2177  35600\n",
      "      B1        0.5   0.012676 0.907549 0.025002 2140 166686\n",
      "      B2        0.5   0.067557 0.923810 0.125907 2134  29454\n",
      "      B1        0.7   0.015821 0.883800 0.031086 2084 129638\n",
      "      B2        0.7   0.080366 0.900866 0.147568 2081  23813\n"
     ]
    }
   ],
   "source": [
    "# Create threshold comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for i, thresh in enumerate(thresholds):\n",
    "    comparison_data.append({\n",
    "        'Blocking': 'B1',\n",
    "        'Threshold': thresh,\n",
    "        'Precision': B1_results[i]['precision'],\n",
    "        'Recall': B1_results[i]['recall'],\n",
    "        'F1': B1_results[i]['f1'],\n",
    "        'TP': B1_results[i]['tp'],\n",
    "        'FP': B1_results[i]['fp']\n",
    "    })\n",
    "    comparison_data.append({\n",
    "        'Blocking': 'B2',\n",
    "        'Threshold': thresh,\n",
    "        'Precision': B2_results[i]['precision'],\n",
    "        'Recall': B2_results[i]['recall'],\n",
    "        'F1': B2_results[i]['f1'],\n",
    "        'TP': B2_results[i]['tp'],\n",
    "        'FP': B2_results[i]['fp']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(os.path.join(DITTO_RESULTS_DIR, 'threshold_comparison.csv'), index=False)\n",
    "\n",
    "print(\"\\nThreshold Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "185d8dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 4.G COMPLETED\n",
      "============================================================\n",
      "All results saved to: c:\\Users\\migli\\HW6ID\\ditto_results\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 4.G COMPLETED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"All results saved to: {DITTO_RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
